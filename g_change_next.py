import streamlit as st
import pandas as pd
import re
import io
import os
import unicodedata
from openpyxl import load_workbook
from openpyxl.styles import PatternFill

# =========================
# ãƒšãƒ¼ã‚¸è¨­å®šï¼ã‚¹ã‚¿ã‚¤ãƒ«
# =========================
st.set_page_config(page_title="G-Change Next", layout="wide")
st.markdown("""
    <style>
    h1 { color: #800000; }
    </style>
""", unsafe_allow_html=True)
st.title("ğŸš— G-Change Nextï½œä¼æ¥­æƒ…å ±æ•´å½¢ï¼†NGé™¤å¤–ãƒ„ãƒ¼ãƒ«ï¼ˆVer4.6ï¼‰")

# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆæ­£è¦åŒ–ç³»ï¼‰
# =========================
def nfkc(s: str) -> str:
    return unicodedata.normalize("NFKC", s)

def normalize_text(x) -> str:
    """å…±é€šã®è»½é‡æ­£è¦åŒ–ï¼šNFKCã€ç©ºç™½ãƒ»å„ç¨®ãƒ€ãƒƒã‚·ãƒ¥çµ±ä¸€ã€å‰å¾Œç©ºç™½é™¤å»"""
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return ""
    s = str(x).replace("\u3000", " ").replace("\xa0", " ")
    s = re.sub(r'[âˆ’â€“â€”â€•ãƒ¼]', '-', s)
    s = nfkc(s).strip()
    return s

def hiragana_to_katakana(s: str) -> str:
    """ã²ã‚‰ãŒãªâ†’ã‚«ã‚¿ã‚«ãƒŠï¼ˆâ€œç¿»å­—â€ã§ã¯ãªãã€è¡¨è¨˜æºã‚ŒæŠ‘åˆ¶ã®ãŸã‚ã®åŒç³»çµ±çµ±ä¸€ï¼‰"""
    res = []
    for ch in s:
        code = ord(ch)
        if 0x3041 <= code <= 0x3096:  # ã²ã‚‰ãŒãªç¯„å›²
            res.append(chr(code + 0x60))  # ã‚«ã‚¿ã‚«ãƒŠã¸
        else:
            res.append(ch)
    return "".join(res)

# ä¼šç¤¾ç¨®åˆ¥èªï¼ˆæ¯”è¼ƒç”¨ã«é™¤å»ï¼‰
COMPANY_SUFFIXES = [
    "æ ªå¼ä¼šç¤¾", "(æ ª)", "ï¼ˆæ ªï¼‰", "æœ‰é™ä¼šç¤¾", "(æœ‰)", "ï¼ˆæœ‰ï¼‰",
    "inc.", "inc", "co.,ltd.", "co.,ltd", "co.ltd.", "co.ltd", "ltd.", "ltd",
    "corp.", "corp", "co.", "co",
    "åˆåŒä¼šç¤¾", "åˆåä¼šç¤¾", "åˆè³‡ä¼šç¤¾"
]

def canonical_company_name(name: str) -> str:
    """
    ä¼æ¥­åã®â€œæ¯”è¼ƒç”¨ã‚­ãƒ¼â€ã‚’ä½œã‚‹å¼·ã„æ­£è¦åŒ–
    - NFKC/ç©ºç™½æ•´å½¢ãªã©ï¼ˆnormalize_textï¼‰
    - ã²ã‚‰â†’ã‚«ãƒŠçµ±ä¸€ï¼ˆç¿»å­—ã¯ã—ãªã„ï¼è‹±å­—â†’ã‚«ãƒŠç­‰ã¯è¡Œã‚ãªã„ï¼‰
    - è‹±å­—ã®å¤§å°ç„¡è¦–ï¼ˆcasefoldï¼‰
    - ä¼šç¤¾ç¨®åˆ¥èªã®é™¤å»
    - è¨˜å·ãƒ»è£…é£¾ã®é™¤å»ï¼ˆæ¯”è¼ƒç”¨ã«æœ€å°é™ï¼‰
    """
    s = normalize_text(name)
    s = hiragana_to_katakana(s)
    s = s.casefold()  # è‹±å­—ã®å¤§å°ã‚†ã‚Œå¸å
    for suf in sorted(COMPANY_SUFFIXES, key=len, reverse=True):
        s = s.replace(suf.casefold(), "")
    # è¨˜å·ãƒ»ç©ºç™½é¡ã®å‰Šé™¤ï¼ˆæ¯”è¼ƒç”¨ï¼‰
    s = re.sub(r"[\s\-â€“â€”â€•â€ãƒ¼ãƒ»/,.Â·ï½¥\(\)ï¼ˆï¼‰\[\]{}ã€ã€‘ï¼†&ï¼‹+_|]", "", s)
    return s

# é›»è©±ç•ªå·æ­£è¦åŒ–
Z2H_HYPHEN = str.maketrans({
    'ï¼':'-','ãƒ¼':'-','â€':'-','-':'-','â€’':'-','â€“':'-','â€”':'-','â€•':'-'
})

def normalize_phone(raw: str) -> str:
    """
    è¡¨ç¤ºç”¨ã®è»½ã„æ•´å½¢ï¼ˆä¾‹: 03-1234-5678 å½¢å¼ã¸å¯„ã›ã‚‹ï¼‰ã€‚
    â€»æ¯”è¼ƒã¯ phone_digits_only() ã‚’ç”¨ã„ã‚‹ï¼ˆã“ã¡ã‚‰ã¯è¦‹ãŸç›®ã®æ•´å½¢ï¼‰
    """
    if not raw:
        return ""
    s = nfkc(raw).translate(Z2H_HYPHEN)
    s = s.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
    s = re.sub(r"\s+", "", s)  # ç©ºç™½é™¤å»
    s = re.sub(r"(\(å†…ç·š.*?\)|\(ä»£\)|\(ä»£è¡¨\))", "", s)  # å†…ç·šè¡¨è¨˜ãªã©é™¤å»
    s = re.sub(r"^\+81", "0", s)  # å›½ç•ªå·ã‚’0ã¸
    s = re.sub(r"[^\d-]", "", s)  # æ•°å­—ã¨ãƒã‚¤ãƒ•ãƒ³ä»¥å¤–é™¤å»

    digits = re.sub(r"\D", "", s)
    if len(digits) < 9:
        return ""  # æ¡ãŒçŸ­ã™ãã‚‹ã‚‚ã®ã¯ç„¡åŠ¹æ‰±ã„ï¼ˆå¿…è¦ãªã‚‰èª¿æ•´ï¼‰

    # ã–ã£ãã‚Šæ•´å½¢ï¼ˆå³å¯†ãªå¸‚å¤–å±€ç•ªåˆ¤å®šã¯ã—ãªã„ï¼‰
    if len(digits) == 10:
        return f"{digits[0:3]}-{digits[3:6]}-{digits[6:]}"
    if len(digits) == 11:
        return f"{digits[0:3]}-{digits[3:7]}-{digits[7:]}"
    return s  # æƒ³å®šå¤–æ¡ã¯ãã®ã¾ã¾

def phone_digits_only(s: str) -> str:
    """æ¯”è¼ƒç”¨ï¼šæ•°å­—ã®ã¿"""
    return re.sub(r"\D", "", s or "")

def clean_address(address):
    """ä½æ‰€ã®è»½ã„æ•´å½¢ï¼ˆä¸­é»’ç³»ã§åˆ†å‰²â†’å¾Œæ®µå„ªå…ˆï¼‰"""
    address = normalize_text(address)
    split_pattern = r"[Â·ï½¥ãƒ»]"
    if re.search(split_pattern, address):
        return re.split(split_pattern, address)[-1].strip()
    return address

def extract_industry(line):
    parts = re.split(r"[Â·ãƒ»]", normalize_text(line))
    return parts[-1].strip() if len(parts) > 1 else (normalize_text(line))

# =========================
# æ—¢å­˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å®šç¾©ï¼ˆç¾çŠ¶ç¶­æŒï¼‰
# =========================
remove_exact = [
    "ã‚ªãƒ•ã‚£ã‚¹æ©Ÿå™¨ãƒ¬ãƒ³ã‚¿ãƒ«æ¥­", "è¶³å ´ãƒ¬ãƒ³ã‚¿ãƒ«ä¼šç¤¾", "é›»æ°—å·¥", "å»ƒæ£„ç‰©ãƒªã‚µã‚¤ã‚¯ãƒ«æ¥­",
    "ãƒ—ãƒ­ãƒ‘ãƒ³è²©å£²æ¥­è€…", "çœ‹æ¿å°‚é–€åº—", "çµ¦æ°´è¨­å‚™å·¥å ´", "è­¦å‚™æ¥­", "å»ºè¨­ä¼šç¤¾",
    "å·¥å‹™åº—", "å†™çœŸåº—", "äººææ´¾é£æ¥­", "æ•´å‚™åº—", "å€‰åº«", "è‚‰åº—", "ç±³è²©å£²åº—",
    "ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒãƒ¼ã‚±ãƒƒãƒˆ", "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã‚µãƒ¼ãƒ“ã‚¹", "å»ºæåº—",
    "è‡ªå‹•è»Šæ•´å‚™å·¥å ´", "è‡ªå‹•è»Šè²©å£²åº—", "è»Šä½“æ•´å‚™åº—", "å”ä¼š/çµ„ç¹”", "å»ºè¨­è«‹è² æ¥­è€…", "é›»å™¨åº—", "å®¶é›»é‡è²©åº—", "å»ºç¯‰ä¼šç¤¾", "ãƒã‚¦ã‚¹ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¥­", "ç„¼è‚‰åº—",
    "å»ºç¯‰è¨­è¨ˆäº‹å‹™æ‰€","å·¦å®˜","ä½œæ¥­æœåº—","ç©ºèª¿è¨­å‚™å·¥äº‹æ¥­è€…","é‡‘å±ã‚¹ã‚¯ãƒ©ãƒƒãƒ—æ¥­è€…","å®³ç£é§†é™¤ã‚µãƒ¼ãƒ“ã‚¹","ãƒ¢ãƒ¼ã‚¿ãƒ¼ä¿®ç†åº—","ã‚¢ãƒ¼ãƒã‚§ãƒªãƒ¼ã‚·ãƒ§ãƒƒãƒ—","ã‚¢ã‚¹ãƒ™ã‚¹ãƒˆæ¤œæŸ»æ¥­","äº‹å‹™ç”¨å“åº—",
    "æ¸¬é‡å£«","é…ç®¡æ¥­è€…","åŠ´åƒçµ„åˆ","ã‚¬ã‚¹ä¼šç¤¾","ã‚¬ã‚½ãƒªãƒ³ã‚¹ã‚¿ãƒ³ãƒ‰","ã‚¬ãƒ©ã‚¹/ãƒŸãƒ©ãƒ¼åº—","ãƒ¯ã‚¤ãƒŠãƒªãƒ¼","å±‹æ ¹ãµãæ¥­è€…","é«˜ç­‰å­¦æ ¡","é‡‘ç‰©åº—","å²è·¡","å•†å·¥ä¼šè­°æ‰€","æ¸…æƒæ¥­","æ¸…æƒæ¥­è€…","é…ç®¡å·¥"
]
remove_partial = ["è²©å£²åº—", "è²©å£²æ¥­è€…"]

highlight_partial = [
    "é‹è¼¸", "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã‚µãƒ¼ãƒ“ã‚¹", "å€‰åº«", "è¼¸é€ã‚µãƒ¼ãƒ“ã‚¹",
    "é‹é€ä¼šç¤¾ä¼æ¥­ã®ã‚ªãƒ•ã‚£ã‚¹", "é‹é€ä¼šç¤¾"
]

# =========================
# å…¥åŠ›UIï¼ˆç¾çŠ¶ç¶­æŒï¼‰
# =========================
nglist_files = [f for f in os.listdir() if f.endswith(".xlsx") and "NGãƒªã‚¹ãƒˆ" in f]
nglist_options = ["ãªã—"] + [os.path.splitext(f)[0] for f in nglist_files]
selected_nglist = st.selectbox("ğŸ›¡ï¸ ä½¿ç”¨ã™ã‚‹NGãƒªã‚¹ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„", nglist_options)

st.markdown("### ğŸ­ æ¥­ç¨®ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„")
industry_option = st.radio(
    "ã©ã®æ¥­ç¨®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«è©²å½“ã—ã¾ã™ã‹ï¼Ÿ",
    ("è£½é€ æ¥­", "ç‰©æµæ¥­", "ãã®ä»–")
)

uploaded_file = st.file_uploader("ğŸ“¤ æ•´å½¢å¯¾è±¡ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])

# =========================
# è£œåŠ©é–¢æ•°
# =========================
def extract_company_groups(lines):
    """é›»è©±ã‚‰ã—ã„è¡Œã‚’åŸºæº–ã« ä¼æ¥­å/æ¥­ç¨®/ä½æ‰€/é›»è©± ã®é †ã§æ‹¾ã†ç°¡æ˜“ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯"""
    results = []
    rows = [normalize_text(l) for l in lines if normalize_text(l)]
    for i, line in enumerate(rows):
        ph = normalize_phone(line)
        if ph:
            phone = ph
            address = rows[i - 1] if i - 1 >= 0 else ""
            address = clean_address(address)
            industry = extract_industry(rows[i - 2]) if i - 2 >= 0 else ""
            company = rows[i - 3] if i - 3 >= 0 else ""
            results.append([company, industry, address, phone])
    return pd.DataFrame(results, columns=["ä¼æ¥­å", "æ¥­ç¨®", "ä½æ‰€", "é›»è©±ç•ªå·"])

def clean_dataframe(df):
    return df.fillna("").applymap(lambda x: normalize_text(x) if pd.notnull(x) else "")

def remove_empty_rows(df):
    return df[~((df["ä¼æ¥­å"] == "") & (df["æ¥­ç¨®"] == "") & (df["ä½æ‰€"] == "") & (df["é›»è©±ç•ªå·"] == ""))]

# =========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =========================
if uploaded_file:
    filename_no_ext = os.path.splitext(uploaded_file.name)[0]
    xl = pd.ExcelFile(uploaded_file)
    sheet_names = xl.sheet_names

    # 1) å…¥åŠ›ã®èª­ã¿è¾¼ã¿ï¼ˆç¾çŠ¶ç¶­æŒï¼‰
    if "å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼" in sheet_names:
        df_raw = pd.read_excel(uploaded_file, sheet_name="å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼", header=None).fillna("")
        result_df = pd.DataFrame({
            "ä¼æ¥­å": df_raw.iloc[:, 1].astype(str).map(normalize_text),
            "æ¥­ç¨®": df_raw.iloc[:, 2].astype(str).map(normalize_text),
            "ä½æ‰€": df_raw.iloc[:, 3].astype(str).map(clean_address),
            "é›»è©±ç•ªå·": df_raw.iloc[:, 4].astype(str).map(normalize_phone)
        })
    else:
        df = pd.read_excel(uploaded_file, header=None).fillna("")
        lines = df[0].tolist()
        result_df = extract_company_groups(lines)

    result_df = clean_dataframe(result_df)

    # æ¯”è¼ƒç”¨ã‚­ãƒ¼ï¼ˆä¼šç¤¾åãƒ»é›»è©±ï¼‰ã‚’ãƒ‡ãƒ¼ã‚¿å´ã«ã‚‚ä»˜ä¸
    result_df["__company_canon"] = result_df["ä¼æ¥­å"].map(canonical_company_name)
    result_df["__phone_digits"]  = result_df["é›»è©±ç•ªå·"].map(phone_digits_only)

    # 2) æ¥­ç¨®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆç¾çŠ¶ç¶­æŒï¼‰
    removed_by_industry = 0
    styled_df = None
    if industry_option == "è£½é€ æ¥­":
        before = len(result_df)
        result_df = result_df[~result_df["æ¥­ç¨®"].isin(remove_exact)]
        if remove_partial:
            pat = "|".join(map(re.escape, remove_partial))
            result_df = result_df[~result_df["æ¥­ç¨®"].str.contains(pat, na=False)]
        removed_by_industry = before - len(result_df)
        st.warning(f"ğŸ­ è£½é€ æ¥­ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ï¼š{removed_by_industry}ä»¶ã‚’é™¤å¤–ã—ã¾ã—ãŸ")

    elif industry_option == "ç‰©æµæ¥­":
        def highlight_logistics(val):
            v = val or ""
            return "background-color: red" if any(word in v for word in highlight_partial) else ""
        styled_df = result_df.style.applymap(highlight_logistics, subset=["æ¥­ç¨®"])
        st.info("ğŸšš æ¥­ç¨®ãŒä¸€è‡´ã—ãŸã‚»ãƒ«ã‚’èµ¤ããƒã‚¤ãƒ©ã‚¤ãƒˆã—ã¦ã„ã¾ã™ï¼ˆå‡ºåŠ›ã«ã‚‚åæ˜ ï¼‰")

    # 3) NGãƒªã‚¹ãƒˆé©ç”¨ï¼ˆä¼šç¤¾å=éƒ¨åˆ†ä¸€è‡´ï¼é›»è©±=æ•°å­—ä¸€è‡´ï¼‰ï¼‹ 7) å‰Šé™¤ãƒ­ã‚°
    removal_logs = []
    company_removed = 0
    phone_removed = 0

    if selected_nglist != "ãªã—":
        ng_path = f"{selected_nglist}.xlsx"
        if not os.path.exists(ng_path):
            st.error(f"âŒ é¸æŠã•ã‚ŒãŸNGãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼š{ng_path}")
            st.stop()
        ng_df = pd.read_excel(ng_path).fillna("")

        if ng_df.shape[1] < 2:
            st.error("âŒ NGãƒªã‚¹ãƒˆã¯2åˆ—ä»¥ä¸Šå¿…è¦ã§ã™ï¼ˆä¼æ¥­åã€é›»è©±ç•ªå·ï¼‰")
            st.stop()

        # NGå´ã®æ¯”è¼ƒç”¨ã‚­ãƒ¼ã‚’æº–å‚™ï¼ˆä¼šç¤¾åï¼šcanonicalã€é›»è©±ï¼šdigitsï¼‰
        ng_df["__ng_company_canon"] = ng_df.iloc[:, 0].map(canonical_company_name)
        ng_df["__ng_phone_digits"]  = ng_df.iloc[:, 1].astype(str).map(normalize_phone).map(phone_digits_only)

        ng_company_keys = ng_df["__ng_company_canon"].tolist()
        ng_phone_set    = set([p for p in ng_df["__ng_phone_digits"].tolist() if p])

        # 3-a) ä¼æ¥­åï¼ˆcanonical éƒ¨åˆ†ä¸€è‡´ï¼‰ã§å‰Šé™¤
        before = len(result_df)

        def hit_ng_company(canon_name: str):
            # â€»â€œç¿»å­—â€ã¯è¡Œã‚ãšã€canonicalåŒ–ã®ã¿ã§éƒ¨åˆ†ä¸€è‡´
            for ng in ng_company_keys:
                if ng and canon_name and (ng in canon_name or canon_name in ng):
                    return ng
            return None

        hit_indices = []
        for idx, row in result_df.iterrows():
            ng_key = hit_ng_company(row["__company_canon"])
            if ng_key:
                removal_logs.append({
                    "reason": "ng-company",
                    "source_company": row["ä¼æ¥­å"],
                    "source_phone": row["é›»è©±ç•ªå·"],
                    "match_key": row["__company_canon"],
                    "ng_hit": ng_key
                })
                hit_indices.append(idx)

        if hit_indices:
            result_df = result_df.drop(index=hit_indices)

        company_removed = before - len(result_df)

        # 3-b) é›»è©±ï¼ˆæ•°å­—ã ã‘ä¸€è‡´ï¼‰ã§å‰Šé™¤
        before = len(result_df)
        hits = result_df["__phone_digits"].isin(ng_phone_set)
        if hits.any():
            for idx, row in result_df[hits].iterrows():
                removal_logs.append({
                    "reason": "ng-phone",
                    "source_company": row["ä¼æ¥­å"],
                    "source_phone": row["é›»è©±ç•ªå·"],
                    "match_key": row["__phone_digits"],
                    "ng_hit": row["__phone_digits"]
                })
            result_df = result_df[~hits]
        phone_removed = before - len(result_df)

    # 5) é‡è¤‡ã®åŸºæº–ã¯ã€Œé›»è©±ä¸€è‡´ï¼ˆæ•°å­—ã ã‘ï¼‰ã€ã®ã¿
    before = len(result_df)
    dup_mask = result_df["__phone_digits"].ne("").astype(bool) & result_df["__phone_digits"].duplicated(keep="first")
    if dup_mask.any():
        for idx, row in result_df[dup_mask].iterrows():
            removal_logs.append({
                "reason": "phone-duplicate",
                "source_company": row["ä¼æ¥­å"],
                "source_phone": row["é›»è©±ç•ªå·"],
                "match_key": row["__phone_digits"],
                "ng_hit": ""
            })
        result_df = result_df[~dup_mask]
    removed_by_dedup = before - len(result_df)

    # ç©ºè¡Œé™¤å»ï¼ˆç¾çŠ¶ç¶­æŒï¼‰
    result_df = remove_empty_rows(result_df)

    # 6) ä¸¦ã¹æ›¿ãˆã¯ã€Œç©ºé›»è©±ã¯æœ€å¾Œã€â†’é›»è©±æ•°å­—â†’ä¼æ¥­åï¼ˆç¾çŠ¶ç¶­æŒï¼‰
    result_df["_phdigits"] = result_df["__phone_digits"]
    result_df["_is_empty_phone"] = (result_df["_phdigits"] == "")
    result_df = result_df.sort_values(by=["_is_empty_phone", "_phdigits", "ä¼æ¥­å"]).drop(columns=["_phdigits","_is_empty_phone"])
    result_df = result_df.reset_index(drop=True)

    # ç”»é¢è¡¨ç¤ºï¼ˆç¾çŠ¶ç¶­æŒï¼ç‰©æµã¯ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼‰
    st.success(f"âœ… æ•´å½¢å®Œäº†ï¼š{len(result_df)}ä»¶ã®ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    if industry_option == "ç‰©æµæ¥­" and styled_df is not None:
        st.dataframe(styled_df, use_container_width=True)
    else:
        st.dataframe(result_df[["ä¼æ¥­å","æ¥­ç¨®","ä½æ‰€","é›»è©±ç•ªå·"]], use_container_width=True)

    # 7) å®Ÿè¡Œã‚µãƒãƒªãƒ¼ï¼‹å‰Šé™¤ãƒ­ã‚°ã®è¡¨ç¤ºãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    with st.expander("ğŸ“Š å®Ÿè¡Œã‚µãƒãƒªãƒ¼ï¼ˆè©³ç´°ï¼‰"):
        st.markdown(f"""
- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é™¤å¤–ï¼ˆè£½é€ æ¥­ å®Œå…¨ä¸€è‡´ï¼‹ä¸€éƒ¨éƒ¨åˆ†ä¸€è‡´ï¼‰: **{removed_by_industry}** ä»¶  
- NGï¼ˆä¼æ¥­åãƒ»éƒ¨åˆ†ä¸€è‡´ï¼‰å‰Šé™¤: **{company_removed}** ä»¶  
- NGï¼ˆé›»è©±ãƒ»æ•°å­—ä¸€è‡´ï¼‰å‰Šé™¤: **{phone_removed}** ä»¶  
- é‡è¤‡ï¼ˆé›»è©±ãƒ»æ•°å­—ä¸€è‡´ï¼‰å‰Šé™¤: **{removed_by_dedup}** ä»¶  
""")
        if removal_logs:
            log_df = pd.DataFrame(removal_logs)
            st.dataframe(log_df.head(100), use_container_width=True)  # ç”»é¢ã¯ä¸Šä½100ä»¶ã®ã¿
            csv_bytes = log_df.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                "ğŸ§¾ å‰Šé™¤ãƒ­ã‚°ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=csv_bytes,
                file_name="removal_logs.csv",
                mime="text/csv"
            )

    # 8) Excel å‡ºåŠ›ï¼ˆç‰©æµãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’Excelã«ã‚‚åæ˜ ï¼šç¾çŠ¶ç¶­æŒï¼‰
    template_file = "template.xlsx"
    if not os.path.exists(template_file):
        st.error("âŒ template.xlsx ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        st.stop()

    workbook = load_workbook(template_file)
    if "å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼" not in workbook.sheetnames:
        st.error("âŒ template.xlsx ã«ã€å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼ã€ã¨ã„ã†ã‚·ãƒ¼ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    sheet = workbook["å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼"]
    # Bã€œEåˆ—ã®ã¿ã‚¯ãƒªã‚¢ï¼ˆç¾çŠ¶ç¶­æŒï¼ä»–åˆ—ã¯è§¦ã‚‰ãªã„ï¼‰
    for row in sheet.iter_rows(min_row=2, max_row=sheet.max_row):
        for cell in row[1:5]:
            cell.value = None
            cell.fill = PatternFill(fill_type=None)

    red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")

    def is_logi(val: str) -> bool:
        v = val or ""
        return any(word in v for word in highlight_partial)

    for idx, row in result_df.iterrows():
        r = idx + 2
        sheet.cell(row=r, column=2, value=row["ä¼æ¥­å"])
        sheet.cell(row=r, column=3, value=row["æ¥­ç¨®"])
        sheet.cell(row=r, column=4, value=row["ä½æ‰€"])
        sheet.cell(row=r, column=5, value=row["é›»è©±ç•ªå·"])
        if industry_option == "ç‰©æµæ¥­" and is_logi(row["æ¥­ç¨®"]):
            sheet.cell(row=r, column=3).fill = red_fill

    output = io.BytesIO()
    workbook.save(output)
    output.seek(0)

    st.download_button(
        label="ğŸ“¥ æ•´å½¢æ¸ˆã¿ãƒªã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=output,
        file_name=f"{filename_no_ext}ãƒªã‚¹ãƒˆ.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
else:
    st.info("template.xlsx ã¨ï¼ˆå¿…è¦ãªã‚‰ï¼‰NGãƒªã‚¹ãƒˆxlsxã‚’åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«ç½®ã„ã¦ã‹ã‚‰ã€Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
