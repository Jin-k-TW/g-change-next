import streamlit as st
import pandas as pd
import re
import unicodedata
import io
import os
from pathlib import Path
from openpyxl import load_workbook
from openpyxl.styles import PatternFill
from openpyxl.worksheet.datavalidation import DataValidation

# ===============================
# ç°¡æ˜“ãƒ­ã‚°ã‚¤ãƒ³ï¼ˆãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼ï¼‰
# ===============================
def check_password():
    """st.secrets['password'] ã¨ä¸€è‡´ã™ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹ç°¡æ˜“ãƒ­ã‚°ã‚¤ãƒ³"""
    def password_entered():
        """ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«å…¥åŠ›ã•ã‚ŒãŸãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œè¨¼"""
        if "password" not in st.secrets:
            st.session_state["password_correct"] = False
            st.session_state["password_error"] = "ã‚µãƒ¼ãƒãƒ¼å´ã«ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç®¡ç†è€…ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            return

        if st.session_state["password"] == st.secrets["password"]:
            st.session_state["password_correct"] = True
            st.session_state.pop("password", None)  # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰æ–‡å­—åˆ—ã¯æ¶ˆã—ã¦ãŠã
            st.session_state.pop("password_error", None)
        else:
            st.session_state["password_correct"] = False
            st.session_state["password_error"] = "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™ã€‚ã‚‚ã†ä¸€åº¦å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"

    # åˆå›ï¼šãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›æ¬„ã‚’è¡¨ç¤º
    if "password_correct" not in st.session_state:
        st.session_state["password_correct"] = False

    if not st.session_state["password_correct"]:
        st.title("ğŸ” G-Change Next ãƒ­ã‚°ã‚¤ãƒ³")
        st.text_input(
            "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            type="password",
            on_change=password_entered,
            key="password",
        )
        if "password_error" in st.session_state and st.session_state["password_error"]:
            st.error(st.session_state["password_error"])
        # ã“ã“ã§å‡¦ç†ã‚’ã‚¹ãƒˆãƒƒãƒ—ï¼ˆã‚¢ãƒ—ãƒªæœ¬ä½“ã¯ã¾ã è¡¨ç¤ºã—ãªã„ï¼‰
        return False

    # èªè¨¼æ¸ˆã¿
    return True


# ===============================
# Streamlitè¨­å®š
# ===============================
st.set_page_config(page_title="G-Change Next", layout="wide")

# â–¼ã“ã“ã§ãƒ­ã‚°ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ã€‚å¤±æ•—ã—ãŸã‚‰ä»¥é™ã®å‡¦ç†ã¯å®Ÿè¡Œã•ã‚Œãªã„
if not check_password():
    st.stop()

st.title("ğŸš— G-Change Nextï½œä¼æ¥­æƒ…å ±æ•´å½¢ï¼†NGé™¤å¤–ãƒ„ãƒ¼ãƒ«ï¼ˆVer6.3 è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‹ç¢ºå®šãƒœã‚¿ãƒ³çœç•¥ç‰ˆï¼‰")

# ===============================
# ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–
# ===============================
def nfkc(s: str) -> str:
    return unicodedata.normalize("NFKC", s)

def normalize_text(x):
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return ""
    s = str(x).replace("\u3000", " ").replace("\xa0", " ")
    s = re.sub(r'[âˆ’â€“â€”â€•ãƒ¼]', '-', s)
    return nfkc(s).strip()

def clean_address(address: str) -> str:
    address = normalize_text(address)
    return address.strip()

def extract_industry(line: str) -> str:
    return normalize_text(line)

# ===============================
# ä¼æ¥­åæ­£è¦åŒ–ï¼ˆNGç…§åˆç”¨ï¼‰
# ===============================
COMPANY_SUFFIXES = ["æ ªå¼ä¼šç¤¾", "(æ ª)", "ï¼ˆæ ªï¼‰", "æœ‰é™ä¼šç¤¾", "(æœ‰)", "ï¼ˆæœ‰ï¼‰", "åˆåŒä¼šç¤¾"]
def canonical_company_name(name: str) -> str:
    s = normalize_text(name)
    for suf in sorted(COMPANY_SUFFIXES, key=len, reverse=True):
        s = s.replace(suf, "")
    s = re.sub(r"[\s\-ãƒ»/,.Â·ï½¥\(\)ï¼ˆï¼‰ã€ã€‘ï¼†&ï¼‹+_|]", "", s)
    return s

# ===============================
# é›»è©±ç•ªå·å‡¦ç†ï¼ˆåŸæ–‡ä¿æŒï¼‰
# ===============================
HYPHENS = "-â€’â€“â€”â€•âˆ’ï¼ãƒ¼â€ï¹£\u2011"
HYPHENS_CLASS = re.escape(HYPHENS)

# é›»è©±ç•ªå·å€™è£œæŠ½å‡ºï¼ˆèª¤æ¤œå‡ºé˜²æ­¢ï¼‰: æ•°å­—ï¼‹ãƒã‚¤ãƒ•ãƒ³/ç©ºç™½ãŒç¶šã8æ–‡å­—ä»¥ä¸Šã®å¡Š
CANDIDATE_RE = re.compile(rf"[+]?\d(?:[\d{HYPHENS_CLASS}\s]{{6,}})\d")

def pick_phone_token_raw(line: str) -> str:
    """1è¡Œã‹ã‚‰é›»è©±ç•ªå·ã‚‰ã—ã„æ–‡å­—åˆ—ã‚’æŠ½å‡ºã€‚digits é•·ãŒ 9ã€œ11 ä»¥å¤–ã¯ä¸æ¡ç”¨ã€‚åŸæ–‡è¡¨è¨˜ï¼ˆãƒã‚¤ãƒ•ãƒ³ä½ç½®ï¼‰ã‚’ãã®ã¾ã¾è¿”ã™ã€‚"""
    if not line:
        return ""
    s = unicodedata.normalize("NFKC", str(line))
    raw_cands = CANDIDATE_RE.findall(s)
    cands = []
    for token in raw_cands:
        tok = token.strip()
        if ":" in tok:           # æ™‚åˆ»æ··å…¥ãªã©ã¯é™¤å¤–
            continue
        digits = re.sub(r"\D", "", tok)
        if not (9 <= len(digits) <= 11):
            continue             # 11-10 ã®ã‚ˆã†ãªçŸ­ã„å¡Šã¯é™¤å¤–
        if not (digits.startswith("0") or digits.startswith("81")):
            continue             # å›½å†…å…ˆé ­0 or å›½ç•ªå·81ã®ã¿è¨±å¯
        score = (len(digits), tok.count("-"))  # é•·ã„digitsï¼†ãƒã‚¤ãƒ•ãƒ³å¤šã„ï¼é›»è©±ã£ã½ã„
        cands.append((score, tok))
    if not cands:
        return ""
    cands.sort(key=lambda x: x[0], reverse=True)
    return cands[0][1]

def phone_digits_only(s: str) -> str:
    """å†…éƒ¨ç…§åˆç”¨ã«æ•°å­—ã ã‘æŠ½å‡ºï¼ˆåŸæ–‡è¡¨è¨˜ã¯ä¿æŒï¼‰"""
    return re.sub(r"\D", "", str(s or ""))

# ===============================
# æŠ½å‡ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ—¢å­˜3æ–¹å¼ï¼‰
# ===============================
# 1) Googleæ¤œç´¢ãƒªã‚¹ãƒˆï¼ˆç¸¦èª­ã¿ãƒ»é›»è©±ä¸Šä¸‹ï¼‰
def extract_google_vertical(lines):
    results = []
    rows = [str(l) for l in lines if str(l).strip() != ""]
    for i, line in enumerate(rows):
        ph_raw = pick_phone_token_raw(line)
        if ph_raw:
            phone = ph_raw  # åŸæ–‡ä¿æŒ
            address = rows[i - 1] if i - 1 >= 0 else ""
            industry = extract_industry(rows[i - 2]) if i - 2 >= 0 else ""
            company = rows[i - 3] if i - 3 >= 0 else ""
            results.append([company, industry, clean_address(address), phone])
    return pd.DataFrame(results, columns=["ä¼æ¥­å", "æ¥­ç¨®", "ä½æ‰€", "é›»è©±ç•ªå·"])

# 2) ã‚·ã‚´ãƒˆã‚¢ãƒ«ãƒ¯ï¼ˆç¸¦ç©ã¿ï¼‰
def extract_shigoto_arua(df_like: pd.DataFrame) -> pd.DataFrame:
    df = df_like.copy()
    if df.columns.size > 2:
        df = df.iloc[:, :2]
    df.columns = ["col0", "col1"]
    df = df.fillna("")
    current = {"ä¼æ¥­å": "", "ä½æ‰€": "", "é›»è©±ç•ªå·": "", "æ¥­ç¨®": ""}
    out = []

    def flush():
        if current["ä¼æ¥­å"]:
            out.append([current["ä¼æ¥­å"], current["æ¥­ç¨®"], current["ä½æ‰€"], current["é›»è©±ç•ªå·"]])
        current.update({"ä¼æ¥­å": "", "ä½æ‰€": "", "é›»è©±ç•ªå·": "", "æ¥­ç¨®": ""})

    for _, row in df.iterrows():
        k, v = str(row["col0"]), str(row["col1"])
        if k in ["ä½æ‰€", "æ‰€åœ¨åœ°", "æœ¬ç¤¾æ‰€åœ¨åœ°"]:
            current["ä½æ‰€"] = clean_address(v)
        elif k in ["é›»è©±", "é›»è©±ç•ªå·", "TEL", "Tel", "tel"]:
            current["é›»è©±ç•ªå·"] = v  # åŸæ–‡ä¿æŒ
        elif k in ["æ¥­ç¨®", "äº‹æ¥­å†…å®¹", "ç”£æ¥­åˆ†é¡", "è£½é€ æ¥­ç¨®"]:
            current["æ¥­ç¨®"] = extract_industry(v)
        elif k and not v:
            if current["ä¼æ¥­å"]:
                flush()
            current["ä¼æ¥­å"] = k
    if current["ä¼æ¥­å"]:
        flush()
    return pd.DataFrame(out, columns=["ä¼æ¥­å", "æ¥­ç¨®", "ä½æ‰€", "é›»è©±ç•ªå·"])

# 3) æ—¥æœ¬å€‰åº«å”ä¼šï¼ˆ4åˆ—ï¼‰
def extract_warehouse_association(df_like: pd.DataFrame) -> pd.DataFrame:
    df = df_like.fillna("")
    if df.shape[1] < 2:
        return pd.DataFrame(columns=["ä¼æ¥­å", "æ¥­ç¨®", "ä½æ‰€", "é›»è©±ç•ªå·"])
    while df.shape[1] < 4:
        df[f"__pad{df.shape[1]}"] = ""
    df = df.iloc[:, :4]
    df.columns = ["c0", "c1", "c2", "c3"]

    tel_re = re.compile(r"(?:TEL|Tel|tel)\s*([0-9ï¼-ï¼™\-ãƒ¼ï¼\s]+)")
    out, current = [], {"ä¼æ¥­å": "", "ä½æ‰€": "", "é›»è©±ç•ªå·": "", "æ¥­ç¨®_set": set()}

    def flush():
        if current["ä¼æ¥­å"]:
            out.append([current["ä¼æ¥­å"], "ãƒ»".join(current["æ¥­ç¨®_set"]), current["ä½æ‰€"], current["é›»è©±ç•ªå·"]])
        current.update({"ä¼æ¥­å": "", "ä½æ‰€": "", "é›»è©±ç•ªå·": "", "æ¥­ç¨®_set": set()})

    for _, r in df.iterrows():
        if r["c0"]:
            if current["ä¼æ¥­å"] and r["c0"] != current["ä¼æ¥­å"]:
                flush()
            current["ä¼æ¥­å"] = r["c0"]
        if r["c1"]:
            current["ä½æ‰€"] = clean_address(r["c1"])
        if r["c2"]:
            m = tel_re.search(r["c2"])
            if m:
                current["é›»è©±ç•ªå·"] = m.group(1).strip()  # åŸæ–‡ä¿æŒ
        if r["c3"]:
            current["æ¥­ç¨®_set"].add(extract_industry(r["c3"]))
    if current["ä¼æ¥­å"]:
        flush()
    return pd.DataFrame(out, columns=["ä¼æ¥­å", "æ¥­ç¨®", "ä½æ‰€", "é›»è©±ç•ªå·"])


# ===============================
# â˜… æ–°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ãƒ»æ¥­ç¨®ï¼‹ä½æ‰€åŒã‚»ãƒ«ï¼‰
# ===============================
JP_LOC_PATTERN = re.compile(r"(ä¸ç›®|ç•ªåœ°?|å·|å¸‚|åŒº|ç”º|æ‘|éƒ¡|çœŒ|åºœ|é“)")

def is_hours_or_business_line(text: str) -> bool:
    """å–¶æ¥­æ™‚é–“ãƒ»è¨ºç™‚æ™‚é–“ç³»ã®è¡Œã‹ã©ã†ã‹ï¼ˆä½æ‰€å€™è£œã‹ã‚‰ã¯é™¤å¤–ï¼‰"""
    t = normalize_text(text)
    if not t:
        return False
    keywords = [
        "å–¶æ¥­æ™‚é–“", "å–¶æ¥­ä¸­", "å–¶æ¥­æ™‚é–“å¤–", "å–¶æ¥­é–‹å§‹",
        "ã¾ã‚‚ãªãå–¶æ¥­é–‹å§‹", "è¨ºç™‚æ™‚é–“", "è¨ºå¯Ÿæ™‚é–“", "24æ™‚é–“å–¶æ¥­",
    ]
    return any(k in t for k in keywords)

def is_address_like(text: str) -> bool:
    """ä½æ‰€ã‚‰ã—ã„ã‹ã©ã†ã‹ã®ã‚†ã‚‹ã„åˆ¤å®šï¼ˆGoogleç¸¦å‹ã®æ—§ãƒ­ã‚¸ãƒƒã‚¯ç”¨ï¼‰"""
    t = normalize_text(text)
    if not t:
        return False

    # â˜… å–¶æ¥­æ™‚é–“ç³»ã®è¡Œã¯ä½æ‰€æ‰±ã„ã—ãªã„
    if is_hours_or_business_line(t):
        return False

    has_digit = bool(re.search(r"\d", t))
    has_loc_word = bool(JP_LOC_PATTERN.search(t))
    has_block = bool(re.search(r"\d{1,3}[-ï¼ãƒ¼â€]\d{1,3}", t))

    if has_digit and (has_loc_word or has_block):
        return True

    # æ•°å­—ãŒãªãã¦ã‚‚ã€Œâ—‹â—‹å¸‚ã€ã€Œâ—‹â—‹ç”ºã€ãªã©ä½æ‰€èªã ã‘ã®ã‚±ãƒ¼ã‚¹ã‚’å¼±ã‚ã«è¨±å¯
    if has_loc_word and not has_digit:
        return True

    return False

def split_industry_address(text: str):
    """ã‚»ãƒ«å†…ã®å³ç«¯ã®ã€ŒÂ·/ãƒ»/ï½¥ã€ã§æ¥­ç¨®ã¨ä½æ‰€ã«åˆ†å‰²"""
    t = normalize_text(text)
    if not t:
        return "", ""
    # å³ã‹ã‚‰1ã¤ç›®ã®åŒºåˆ‡ã‚Šã‚’æ¢ã™
    last_pos = -1
    for ch in ["Â·", "ãƒ»", "ï½¥"]:
        p = t.rfind(ch)
        if p > last_pos:
            last_pos = p
    if last_pos == -1:
        # åŒºåˆ‡ã‚ŠãŒãªã‘ã‚Œã°å…¨ä½“ã‚’ä½æ‰€æ‰±ã„
        return "", t.strip()
    left = t[:last_pos].strip()
    right = t[last_pos + 1 :].strip()
    if not right:
        # å³å´ãŒç©ºãªã‚‰ä½æ‰€æ‰±ã„ã«å€’ã™
        return "", left
    return left, right

KANJI_KATA_HIRA = r"\u4E00-\u9FFF\u30A0-\u30FF\u3040-\u309F"

def is_company_candidate(text: str) -> bool:
    """ä¼æ¥­åã¨ã—ã¦ä½¿ãˆãã†ã‹ã©ã†ã‹"""
    s = normalize_text(text)
    if not s:
        return False

    # ç„¡è¦–ã—ãŸã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    noise_words = [
        "ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ", "Web ã‚µã‚¤ãƒˆ", "web ã‚µã‚¤ãƒˆ",
        "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§äºˆç´„",
        "ãƒ«ãƒ¼ãƒˆãƒ»ä¹—æ›", "çµŒè·¯æ¡ˆå†…",
        "å…±æœ‰",
        "å–¶æ¥­ä¸­", "å–¶æ¥­æ™‚é–“", "å–¶æ¥­æ™‚é–“å¤–", "å–¶æ¥­é–‹å§‹",
        "ã¾ã‚‚ãªãå–¶æ¥­é–‹å§‹", "ã‚¯ãƒã‚³ãƒŸã¯ã‚ã‚Šã¾ã›ã‚“",
        "å£ã‚³ãƒŸ", "ã‚¯ãƒã‚³ãƒŸ", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ä»¶ã®",
    ]
    if any(w in s for w in noise_words):
        return False

    # ãƒ¬ãƒ“ãƒ¥ãƒ¼ç‚¹æ•°å½¢å¼: 5.0(1) ãªã©
    if re.match(r"^\d+(?:\.\d+)?\s*\(.+\)\s*$", s):
        return False

    # æ•°å€¤ã‚„è¨˜å·ã®ã¿ (-22, 3.5 ãªã©) ã‚’é™¤å¤–
    if re.match(r"^[\d\.\-ï¼‹\+ãƒã‚¤ãƒŠã‚¹\s]+$", s):
        return False

    # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ãƒ»è‹±å­—ãŒå°‘ãªãã¨ã‚‚1ã¤
    if not re.search(rf"[{KANJI_KATA_HIRA}A-Za-z]", s):
        return False

    return True

def is_google_meta_line(text: str) -> bool:
    """Googleæ¤œç´¢çµæœã«å‡ºã¦ãã‚‹ãƒ¡ã‚¿æƒ…å ±è¡Œã‹ã©ã†ã‹ï¼ˆä½æ‰€ãƒ»æ¥­ç¨®å€™è£œã‹ã‚‰ã¯é™¤å¤–ï¼‰"""
    t = normalize_text(text)
    if not t:
        return True  # ç©ºè¡Œã¯ãƒ¡ã‚¿æ‰±ã„ã§é£›ã°ã™

    meta_keywords = [
        "ãƒ«ãƒ¼ãƒˆãƒ»ä¹—æ›", "çµŒè·¯æ¡ˆå†…",
        "ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆ", "Web ã‚µã‚¤ãƒˆ", "web ã‚µã‚¤ãƒˆ",
        "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§äºˆç´„",
        "å…±æœ‰",
        "ç¾åœ¨å–¶æ¥­ä¸­", "å–¶æ¥­æ™‚é–“", "å–¶æ¥­æ™‚é–“å¤–",
        "å–¶æ¥­é–‹å§‹", "ã¾ã‚‚ãªãå–¶æ¥­é–‹å§‹", "24æ™‚é–“å–¶æ¥­",
        "ã‚¯ãƒã‚³ãƒŸã¯ã‚ã‚Šã¾ã›ã‚“", "å£ã‚³ãƒŸ", "ã‚¯ãƒã‚³ãƒŸ", "ãƒ¬ãƒ“ãƒ¥ãƒ¼",
    ]
    if any(k in t for k in meta_keywords):
        return True

    # æ•°å€¤ã‚„è¨˜å·ã ã‘ã®è¡Œï¼ˆè©•ä¾¡ç‚¹ã€-22 ãªã©ï¼‰
    if re.match(r"^[\d\.\-ï¼‹\+ãƒã‚¤ãƒŠã‚¹\s]+$", t):
        return True

    return False

def extract_google_free_vertical(df_like: pd.DataFrame) -> pd.DataFrame:
    """
    Googleæ¤œç´¢çµæœï¼ˆç¸¦ä¸¦ã³ãƒ»ãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ãƒ»
    ã€Œæ¥­ç¨®ï¼‹ä½æ‰€ã€ãŒåŒã˜ã‚»ãƒ«ã«å…¥ã£ã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰ã‹ã‚‰

      ä¼æ¥­å / æ¥­ç¨® / ä½æ‰€ / é›»è©±ç•ªå·

    ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    ä¼æ¥­åã¯ã€Œé›»è©±ã‹ã‚‰3ã€œ4è¡Œä¸Šã€ã®ãƒ«ãƒ¼ãƒ«ã‚’å„ªå…ˆã—ã¤ã¤ã€
    ãã®é–“ã®è¡Œã‹ã‚‰æ¥­ç¨®ï¼‹ä½æ‰€ã®ã‚»ãƒ«ã‚’æ‹¾ã†ã€‚
    """
    df0 = df_like.fillna("")
    col = df0.iloc[:, 0].astype(str).tolist()
    n = len(col)
    results = []

    for i, line in enumerate(col):
        ph_raw = pick_phone_token_raw(line)
        if not ph_raw:
            continue
        phone = ph_raw

        # --------------------------
        # 1) ä¼æ¥­åã®è¡Œã‚’æ±ºã‚ã‚‹
        # --------------------------
        company_idx = None

        # ã¾ãš Jin ã•ã‚“ãƒ«ãƒ¼ãƒ«ã§å€™è£œã‚’æ±ºã‚ã‚‹
        txt_m2 = normalize_text(col[i - 2]) if i - 2 >= 0 else ""
        if i - 3 >= 0 and "ã‚¯ãƒã‚³ãƒŸã¯ã‚ã‚Šã¾ã›ã‚“" in txt_m2:
            # é›»è©±ã®2è¡Œä¸Šã«ã€Œã‚¯ãƒã‚³ãƒŸã¯ã‚ã‚Šã¾ã›ã‚“ã€â†’ 3è¡Œä¸ŠãŒä¼æ¥­åå€™è£œ
            company_idx = i - 3
        elif i - 4 >= 0:
            # ãã‚Œä»¥å¤–ã¯åŸºæœ¬4è¡Œä¸Š
            company_idx = i - 4

        # å€™è£œãŒä¼šç¤¾åã¨ã—ã¦å¾®å¦™ãªã‚‰ã€ä¸Šæ–¹å‘ã«ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ä¼šç¤¾åã‚‰ã—ã„è¡Œã‚’æ¢ã™
        if company_idx is not None:
            if not is_company_candidate(col[company_idx]):
                company_idx = None

        if company_idx is None:
            for k in range(i - 1, -1, -1):
                if is_company_candidate(col[k]):
                    company_idx = k
                    break

        if company_idx is None:
            # ä¼æ¥­åãŒã©ã†ã—ã¦ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã“ã®é›»è©±ã¯ã‚¹ã‚­ãƒƒãƒ—
            continue

        company = normalize_text(col[company_idx])

        # --------------------------
        # 2) æ¥­ç¨®ï¼‹ä½æ‰€ã‚»ãƒ«ã‚’æ¢ã™
        # --------------------------
        indaddr_idx = None
        # é›»è©±ã®1è¡Œä¸Šã‹ã‚‰ä¼æ¥­åã®1è¡Œä¸‹ã¾ã§ã‚’é€†é †ã«è¦‹ã¦ã€
        # ãƒ¡ã‚¿è¡Œã‚’é£›ã°ã—ãªãŒã‚‰æœ€åˆã«è¦‹ã¤ã‹ã£ãŸè¡Œã‚’æ¡ç”¨
        for j in range(i - 1, company_idx, -1):
            txt = normalize_text(col[j])
            if not txt:
                continue
            if is_google_meta_line(txt):
                continue
            indaddr_idx = j
            break

        # ã©ã†ã—ã¦ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ä¿é™ºã¨ã—ã¦ã€
        # é›»è©±ã®1è¡Œä¸Šã‹ã‚‰ä¸Šæ–¹å‘ã«ãƒ¡ã‚¿ä»¥å¤–ã®è¡Œã‚’æ¢ã™
        if indaddr_idx is None:
            for j in range(i - 1, -1, -1):
                txt = normalize_text(col[j])
                if not txt:
                    continue
                if is_google_meta_line(txt):
                    continue
                indaddr_idx = j
                break

        industry = ""
        address = ""

        if indaddr_idx is not None:
            ind_raw, addr_raw = split_industry_address(col[indaddr_idx])

            if addr_raw:
                # ã€Œæ¥­ç¨®ãƒ»ä½æ‰€ã€ã®ã‚ˆã†ã«åˆ†å‰²ã§ããŸã‚±ãƒ¼ã‚¹
                industry = extract_industry(ind_raw)
                address = clean_address(addr_raw)
            else:
                # åŒºåˆ‡ã‚Šè¨˜å·ãŒç„¡ã„ â†’ å…¨ä½“ã‚’ä½æ‰€æ‰±ã„
                address = clean_address(col[indaddr_idx])

        # --------------------------
        # 3) çµæœã¨ã—ã¦è¿½åŠ 
        # --------------------------
        results.append([company, industry, address, phone])

    if not results:
        return pd.DataFrame(columns=["ä¼æ¥­å", "æ¥­ç¨®", "ä½æ‰€", "é›»è©±ç•ªå·"])

    return pd.DataFrame(results, columns=["ä¼æ¥­å", "æ¥­ç¨®", "ä½æ‰€", "é›»è©±ç•ªå·"])


# ===============================
# æ¥­ç¨®ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼/ãƒã‚¤ãƒ©ã‚¤ãƒˆ
# ===============================
remove_exact = [
    "ã‚ªãƒ•ã‚£ã‚¹æ©Ÿå™¨ãƒ¬ãƒ³ã‚¿ãƒ«æ¥­", "è¶³å ´ãƒ¬ãƒ³ã‚¿ãƒ«ä¼šç¤¾", "é›»æ°—å·¥", "å»ƒæ£„ç‰©ãƒªã‚µã‚¤ã‚¯ãƒ«æ¥­",
    "ãƒ—ãƒ­ãƒ‘ãƒ³è²©å£²æ¥­è€…", "çœ‹æ¿å°‚é–€åº—", "çµ¦æ°´è¨­å‚™å·¥å ´", "è­¦å‚™æ¥­", "å»ºè¨­ä¼šç¤¾",
    "å·¥å‹™åº—", "å†™çœŸåº—", "äººææ´¾é£æ¥­", "æ•´å‚™åº—", "å€‰åº«", "è‚‰åº—", "ç±³è²©å£²åº—",
    "ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒãƒ¼ã‚±ãƒƒãƒˆ", "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã‚µãƒ¼ãƒ“ã‚¹", "å»ºæåº—",
    "è‡ªå‹•è»Šæ•´å‚™å·¥å ´", "è‡ªå‹•è»Šè²©å£²åº—", "è»Šä½“æ•´å‚™åº—", "å”ä¼š/çµ„ç¹”", "å»ºè¨­è«‹è² æ¥­è€…", "é›»å™¨åº—", "å®¶é›»é‡è²©åº—", "å»ºç¯‰ä¼šç¤¾", "ãƒã‚¦ã‚¹ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¥­", "ç„¼è‚‰åº—",
    "å»ºç¯‰è¨­è¨ˆäº‹å‹™æ‰€", "å·¦å®˜", "ä½œæ¥­æœåº—", "ç©ºèª¿è¨­å‚™å·¥äº‹æ¥­è€…", "é‡‘å±ã‚¹ã‚¯ãƒ©ãƒƒãƒ—æ¥­è€…", "å®³ç£é§†é™¤ã‚µãƒ¼ãƒ“ã‚¹", "ãƒ¢ãƒ¼ã‚¿ãƒ¼ä¿®ç†åº—", "ã‚¢ãƒ¼ãƒã‚§ãƒªãƒ¼ã‚·ãƒ§ãƒƒãƒ—", "ã‚¢ã‚¹ãƒ™ã‚¹ãƒˆæ¤œæŸ»æ¥­", "äº‹å‹™ç”¨å“åº—",
    "æ¸¬é‡å£«", "é…ç®¡æ¥­è€…", "åŠ´åƒçµ„åˆ", "ã‚¬ã‚¹ä¼šç¤¾", "ã‚¬ã‚½ãƒªãƒ³ã‚¹ã‚¿ãƒ³ãƒ‰", "ã‚¬ãƒ©ã‚¹/ãƒŸãƒ©ãƒ¼åº—", "ãƒ¯ã‚¤ãƒŠãƒªãƒ¼", "å±‹æ ¹ãµãæ¥­è€…", "é«˜ç­‰å­¦æ ¡", "é‡‘ç‰©åº—", "å²è·¡", "å•†å·¥ä¼šè­°æ‰€", "æ¸…æƒæ¥­", "æ¸…æƒæ¥­è€…", "é…ç®¡å·¥", "ãŠæ‰‹é ƒ"
]
remove_partial = ["è²©å£²åº—", "è²©å£²æ¥­è€…"]

highlight_partial = [
    "é‹è¼¸", "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã‚µãƒ¼ãƒ“ã‚¹", "å€‰åº«", "è¼¸é€ã‚µãƒ¼ãƒ“ã‚¹",
    "é‹é€ä¼šç¤¾ä¼æ¥­ã®ã‚ªãƒ•ã‚£ã‚¹", "é‹é€ä¼šç¤¾"
]

# ===============================
# æ¥­ç¨®ãƒã‚¤ã‚ºé™¤å»ï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼/è©•ä¾¡ãªã©ï¼‰
# ===============================
def clean_industry_noise(s: str) -> str:
    """
    æ¥­ç¨®ã‚«ãƒ©ãƒ ã«ç´›ã‚Œè¾¼ã‚€
    - ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±ï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ãªã—ãƒ»â€¦ï¼‰
    - Google ã®ã‚¯ãƒã‚³ãƒŸ
    - â—‹ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼å£ã‚³ãƒŸ
    ãªã©ã®ãƒã‚¤ã‚ºã‚’é™¤å»ã™ã‚‹
    ï¼‹ æœ€å¾Œã«ã€ŒÂ·ã€ã€Œãƒ¬ãƒ“ãƒ¥-ãªã—ã€ã€Œç©ºç™½ã ã‘ã€ã¯å¿…ãšæ¶ˆã™
    """
    if not s:
        return ""
    t = str(s)
    # ç©ºç™½ã‚’ã‚†ã‚‹ãæ­£è¦åŒ–
    t = re.sub(r"\s+", " ", t).strip()

    # å…ˆé ­ã®è©•ä¾¡ã‚¹ã‚³ã‚¢ + ä»¶æ•° ä¾‹: '4.7(123)ãƒ»', '4.7ï¼ˆ123ï¼‰ãƒ»'
    t = re.sub(r"^\s*\d+(?:\.\d+)?\s*[\(ï¼ˆ]\s*\d+\s*[\)ï¼‰]\s*(?:ä»¶)?\s*[ãƒ»ï½¥]?\s*", "", t)

    # ---- ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ãªã—ãƒ»â—‹â—‹ã€ç³»ã‚’ãƒˆãƒ¼ã‚¯ãƒ³å˜ä½ã§å‡¦ç† ----
    def norm_token(x: str) -> str:
        return re.sub(r"\s+", "", x)

    noise_basic = {"ãƒ¬ãƒ“ãƒ¥ãƒ¼", "ãƒ¬ãƒ“ãƒ¥ãƒ¼ãªã—", "ãƒ¬ãƒ“ãƒ¥ãƒ¼ç„¡ã—", "ã‚¯ãƒã‚³ãƒŸ", "å£ã‚³ãƒŸ"}
    noise_nashi = {"ãªã—"}

    if t.startswith("ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
        parts = [p.strip() for p in re.split(r"[ãƒ»ï½¥]", t) if p.strip()]
        if not parts:
            return ""

        # å…¨éƒ¨ãƒã‚¤ã‚ºãªã‚‰ç©ºã«ã™ã‚‹
        if all(norm_token(p) in noise_basic | noise_nashi for p in parts):
            return ""

        cleaned_parts = []
        for p in parts:
            pn = norm_token(p)
            if pn in noise_basic or pn in noise_nashi:
                continue
            cleaned_parts.append(p)

        t = "ãƒ»".join(cleaned_parts)
    else:
        # ã€ŒGoogle ã®ã‚¯ãƒã‚³ãƒŸã€ã€Œå£ã‚³ãƒŸã€ã€Œã‚¯ãƒã‚³ãƒŸã€ãªã©ãŒé€”ä¸­ã«ã‚ã‚‹å ´åˆ
        t = re.sub(r"(?:^|[ãƒ»ï½¥])\s*(Google\s*ã®?\s*ã‚¯ãƒã‚³ãƒŸ|å£ã‚³ãƒŸ|ã‚¯ãƒã‚³ãƒŸ)\s*(?=[ãƒ»ï½¥]|$)", "", t)
        # ã€Œâ—¯ä»¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ã€Œâ—¯ä»¶ã®å£ã‚³ãƒŸã€ãªã©
        t = re.sub(r"[ãƒ»ï½¥]?\s*\d+\s*ä»¶ã®?(ãƒ¬ãƒ“ãƒ¥ãƒ¼|å£ã‚³ãƒŸ|ã‚¯ãƒã‚³ãƒŸ)\s*(?=[ãƒ»ï½¥]|$)", "", t)

    # åˆ†å‰²ã—ã¦ç©ºè¦ç´ ã‚’å‰Šé™¤
    parts = [p.strip() for p in re.split(r"[ãƒ»ï½¥]", t) if p.strip()]
    t = "ãƒ»".join(parts) if parts else ""

    # ä½™è¨ˆãªåŒºåˆ‡ã‚Šã‚„ç©ºç™½ã‚’æ•´å½¢
    t = re.sub(r"[ãƒ»ï½¥]{2,}", "ãƒ»", t).strip(" ãƒ»ï½¥")

    # â–¼â–¼â–¼ ã“ã“ãŒã€Œå¿…ãšæ¶ˆã™ã€éƒ¨åˆ† â–¼â–¼â–¼
    # ä¸­é»’ã€ŒÂ·ã€ã‚„ã€Œãƒ¬ãƒ“ãƒ¥-ãªã—ã€ã‚’å¼·åˆ¶å‰Šé™¤
    if t:
        for trash in ["Â·", "ãƒ¬ãƒ“ãƒ¥-ãªã—"]:
            t = t.replace(trash, "")
        # ã¤ã„ã§ã«å…¨è§’/åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã ã‘ã«ãªã£ãŸå ´åˆã‚‚ç©ºã«ã™ã‚‹
        t = re.sub(r"\s+", " ", t).strip()

    return t if t else ""

# ===============================
# å…±é€šæ•´å½¢ï¼ˆé›»è©±ã¯è§¦ã‚‰ãªã„ï¼‰
# ===============================
def clean_dataframe_except_phone(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    for c in ["ä¼æ¥­å", "æ¥­ç¨®", "ä½æ‰€"]:
        df[c] = df[c].map(normalize_text)
    df["æ¥­ç¨®"] = df["æ¥­ç¨®"].map(clean_industry_noise)
    return df.fillna("")

# ===============================
# UIï¼ˆNGãƒªã‚¹ãƒˆé¸æŠãƒ»æŠ½å‡ºæ–¹å¼ãƒ»æ¥­ç¨®ã‚«ãƒ†ã‚´ãƒªãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå…¥åŠ›ï¼‰
# ===============================
st.markdown("### ğŸ›¡ï¸ ä½¿ç”¨ã™ã‚‹NGãƒªã‚¹ãƒˆã‚’é¸æŠ")
nglist_files = [f for f in os.listdir() if f.endswith(".xlsx") and "NGãƒªã‚¹ãƒˆ" in f]
nglist_options = ["ãªã—"] + [os.path.splitext(f)[0] for f in nglist_files]
selected_nglist = st.selectbox(
    "NGãƒªã‚¹ãƒˆ",
    nglist_options,
    index=0,
    help="åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚ã‚‹ã€NGãƒªã‚¹ãƒˆã€œ.xlsxã€ã‚’æ¤œå‡ºã—ã¾ã™ã€‚1åˆ—ç›®=ä¼æ¥­åã€2åˆ—ç›®=é›»è©±ç•ªå·ï¼ˆä»»æ„ï¼‰ã€‚"
)

st.markdown("### ğŸ§­ æŠ½å‡ºæ–¹æ³•ã‚’é¸æŠ")
profile = st.selectbox(
    "æŠ½å‡ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«",
    [
        "Googleæ¤œç´¢ãƒªã‚¹ãƒˆï¼ˆç¸¦èª­ã¿ãƒ»é›»è©±ä¸Šä¸‹å‹ï¼‰",
        "Googleæ¤œç´¢ãƒªã‚¹ãƒˆï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ãƒ»æ¥­ç¨®ï¼‹ä½æ‰€åŒã‚»ãƒ«ï¼‰",  # â˜…è¿½åŠ 
        "ã‚·ã‚´ãƒˆã‚¢ãƒ«ãƒ¯æ¤œç´¢ãƒªã‚¹ãƒˆï¼ˆç¸¦ç©ã¿ï¼‰",
        "æ—¥æœ¬å€‰åº«å”ä¼šãƒªã‚¹ãƒˆï¼ˆ4åˆ—å‹ï¼‰",
    ]
)

st.markdown("### ğŸ­ æ¥­ç¨®ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ")
industry_option = st.radio("ã©ã®æ¥­ç¨®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«è©²å½“ã—ã¾ã™ã‹ï¼Ÿ", ("è£½é€ æ¥­", "ç‰©æµæ¥­", "ãã®ä»–"))

st.markdown("### ğŸ§© ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å–å¾—æ–¹æ³•ï¼ˆOSäº’æ›å¼·åŒ–ï¼‰")
template_source = st.radio(
    "template.xlsx ã®å–å¾—å…ƒ",
    ("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã® template.xlsx ã‚’ä½¿ã†ï¼ˆå¾“æ¥ï¼‰", "ã“ã“ã§ template.xlsx ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ã†"),
    index=0
)
template_upload = None
if template_source == "ã“ã“ã§ template.xlsx ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ã†":
    template_upload = st.file_uploader("template.xlsx ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"], key="template_up")

# â˜… è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼šaccept_multiple_files=True
uploaded_files = st.file_uploader(
    "ğŸ“¤ æ•´å½¢å¯¾è±¡ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°é¸æŠå¯ï¼‰",
    type=["xlsx"],
    accept_multiple_files=True
)

# ===============================
# NGãƒªã‚¹ãƒˆã‚’ä¸€åº¦ã ã‘èª­ã¿è¾¼ã‚“ã§å…±æœ‰
# ===============================
ng_names = []
ng_phones = set()
if uploaded_files and selected_nglist != "ãªã—":
    ng_path = f"{selected_nglist}.xlsx"
    if not os.path.exists(ng_path):
        st.error(f"âŒ é¸æŠã•ã‚ŒãŸNGãƒªã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼š{ng_path}")
        st.stop()
    ng_df = pd.read_excel(ng_path, engine="openpyxl").fillna("")
    if ng_df.shape[1] < 1:
        st.error("âŒ NGãƒªã‚¹ãƒˆã¯å°‘ãªãã¨ã‚‚1åˆ—ï¼ˆä¼æ¥­åï¼‰ãŒå¿…è¦ã§ã™ã€‚2åˆ—ç›®ã«é›»è©±ç•ªå·ãŒã‚ã‚Œã°ç…§åˆã«åˆ©ç”¨ã—ã¾ã™ã€‚")
        st.stop()
    ng_df["__ng_company_canon"] = ng_df.iloc[:, 0].map(canonical_company_name)
    if ng_df.shape[1] >= 2:
        ng_df["__ng_digits"] = ng_df.iloc[:, 1].astype(str).map(phone_digits_only)
    else:
        ng_df["__ng_digits"] = ""
    ng_names = [n for n in ng_df["__ng_company_canon"].tolist() if n]
    ng_phones = set([d for d in ng_df["__ng_digits"].tolist() if d])

# ===============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆâ˜…ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ç‹¬ç«‹ã—ã¦å‡¦ç†ï¼‰
# ===============================
if uploaded_files:
    for file_index, uploaded_file in enumerate(uploaded_files):
        st.markdown("---")
        st.markdown(f"## ğŸ“ {uploaded_file.name}")

        filename_no_ext = os.path.splitext(uploaded_file.name)[0]
        xl = pd.ExcelFile(uploaded_file, engine="openpyxl")

        # --- æŠ½å‡º ---
        if "å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼" in xl.sheet_names:
            # templateäº’æ›: å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼ã‹ã‚‰èª­ã¿å–ã‚Šï¼ˆé›»è©±ã¯åŸæ–‡ã®ã¾ã¾ï¼‰
            df_raw = pd.read_excel(
                xl,
                sheet_name="å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼",
                header=None,
                engine="openpyxl"
            ).fillna("")
            df = pd.DataFrame({
                "ä¼æ¥­å": df_raw.iloc[1:, 1].astype(str),
                "æ¥­ç¨®": df_raw.iloc[1:, 2].astype(str),
                "ä½æ‰€": df_raw.iloc[1:, 3].astype(str),
                "é›»è©±ç•ªå·": df_raw.iloc[1:, 4].astype(str),
            })
        else:
            df0 = pd.read_excel(uploaded_file, header=None, engine="openpyxl").fillna("")
            if profile == "Googleæ¤œç´¢ãƒªã‚¹ãƒˆï¼ˆç¸¦èª­ã¿ãƒ»é›»è©±ä¸Šä¸‹å‹ï¼‰":
                lines = df0.iloc[:, 0].tolist()
                df = extract_google_vertical(lines)
            elif profile == "Googleæ¤œç´¢ãƒªã‚¹ãƒˆï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ãƒ»æ¥­ç¨®ï¼‹ä½æ‰€åŒã‚»ãƒ«ï¼‰":
                df = extract_google_free_vertical(df0)
            elif profile == "ã‚·ã‚´ãƒˆã‚¢ãƒ«ãƒ¯æ¤œç´¢ãƒªã‚¹ãƒˆï¼ˆç¸¦ç©ã¿ï¼‰":
                df = extract_shigoto_arua(df0)
            else:
                df = extract_warehouse_association(df0)

        # --- éé›»è©±åˆ—ã®ã¿æ­£è¦åŒ– ---
        df = clean_dataframe_except_phone(df)

        # --- æ¯”è¼ƒã‚­ãƒ¼ ---
        df["__company_canon"] = df["ä¼æ¥­å"].map(canonical_company_name)
        df["__digits"] = df["é›»è©±ç•ªå·"].map(phone_digits_only)

        # --- æ¥­ç¨®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆè£½é€ æ¥­ã®ã¿é™¤å¤–ãƒ«ãƒ¼ãƒ«é©ç”¨ï¼‰ ---
        removed_by_industry = 0
        if industry_option == "è£½é€ æ¥­":
            before = len(df)
            all_ng_words = remove_exact + remove_partial
            if all_ng_words:
                pat = "|".join(map(re.escape, all_ng_words))
                df = df[~df["æ¥­ç¨®"].str.contains(pat, na=False)]
            removed_by_industry = before - len(df)
            st.warning(f"ğŸ­ è£½é€ æ¥­ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ï¼š{removed_by_industry}ä»¶ã‚’é™¤å¤–ã—ã¾ã—ãŸ")

        # --- NGç…§åˆï¼ˆä»»æ„ï¼‰ ---
        removal_logs = []
        company_removed = 0
        phone_removed = 0
        dup_removed = 0

        if ng_names or ng_phones:
            # ä¼æ¥­åï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒ»ç›¸äº’åŒ…å«ï¼‰
            before = len(df)
            hit_idx = []
            for idx, row in df.iterrows():
                c = row["__company_canon"]
                if not c:
                    continue
                if any((n in c or c in n) for n in ng_names):
                    removal_logs.append({
                        "reason": "ng-company",
                        "company": row["ä¼æ¥­å"],
                        "phone_raw": row["é›»è©±ç•ªå·"],
                        "match": c
                    })
                    hit_idx.append(idx)
            if hit_idx:
                df = df.drop(index=hit_idx)
            company_removed = before - len(df)

            # é›»è©±ç•ªå·digitsä¸€è‡´
            before = len(df)
            mask = df["__digits"].isin(ng_phones)
            if mask.any():
                for idx, row in df[mask].iterrows():
                    removal_logs.append({
                        "reason": "ng-phone",
                        "company": row["ä¼æ¥­å"],
                        "phone_raw": row["é›»è©±ç•ªå·"],
                        "match": row["__digits"]
                    })
                df = df[~mask]
            phone_removed = before - len(df)

        # --- é‡è¤‡ï¼ˆé›»è©±digitsï¼‰é™¤å»ï¼ˆâ€»ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«å†…ã ã‘ï¼‰ ---
        before = len(df)
        dup_mask = df["__digits"].ne("").astype(bool) & df["__digits"].duplicated(keep="first")
        if dup_mask.any():
            for idx, row in df[dup_mask].iterrows():
                removal_logs.append({
                    "reason": "dup-phone",
                    "company": row["ä¼æ¥­å"],
                    "phone_raw": row["é›»è©±ç•ªå·"],
                    "match": row["__digits"]
                })
            df = df[~dup_mask]
        dup_removed = before - len(df)

        # --- ç©ºè¡Œã®é™¤å» ---
        df = df[~((df["ä¼æ¥­å"] == "") & (df["æ¥­ç¨®"] == "") & (df["ä½æ‰€"] == "") & (df["é›»è©±ç•ªå·"] == ""))].reset_index(drop=True)

        # --- ç”»é¢è¡¨ç¤ºï¼ˆç·¨é›†å¯ãƒ»ç¢ºå®šãƒœã‚¿ãƒ³ãªã—ï¼‰ ---
        st.success(f"âœ… æ•´å½¢å®Œäº†ï¼š{len(df)}ä»¶ã®ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        edited = st.data_editor(
            df[["ä¼æ¥­å", "æ¥­ç¨®", "ä½æ‰€", "é›»è©±ç•ªå·"]],
            use_container_width=True,
            num_rows="fixed",
            column_config={
                "ä¼æ¥­å": st.column_config.TextColumn(required=True),
                "æ¥­ç¨®": st.column_config.TextColumn(),
                "ä½æ‰€": st.column_config.TextColumn(),
                "é›»è©±ç•ªå·": st.column_config.TextColumn(
                    help="åŸæ–‡ã®é…åˆ—ã‚’ä¿æŒã€‚å¿…è¦ãªã‚‰ã“ã“ã§æ‰‹å‹•ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚ç·¨é›†å†…å®¹ã¯ãã®ã¾ã¾å‡ºåŠ›ã«åæ˜ ã•ã‚Œã¾ã™ã€‚"
                ),
            },
            key=f"editable_preview_{file_index}",
        )

        # ç¢ºå®šãƒœã‚¿ãƒ³ã¯å»ƒæ­¢ã€‚edited ã‚’ãã®ã¾ã¾å‡ºåŠ›ç”¨ã«ä½¿ã†
        df_export = edited.copy()

        # --- ã‚µãƒãƒªãƒ¼ï¼†å‰Šé™¤ãƒ­ã‚°DL ---
        with st.expander(f"ğŸ“Š å®Ÿè¡Œã‚µãƒãƒªãƒ¼ï¼ˆè©³ç´°ï¼‰ - {uploaded_file.name}", expanded=False):
            st.markdown(
                f"- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é™¤å¤–ï¼ˆè£½é€ æ¥­ éƒ¨åˆ†ä¸€è‡´ï¼‰: **{removed_by_industry}** ä»¶\n"
                f"- NGï¼ˆä¼æ¥­å éƒ¨åˆ†ä¸€è‡´ï¼‰å‰Šé™¤: **{company_removed}** ä»¶\n"
                f"- NGï¼ˆé›»è©± digitsä¸€è‡´ï¼‰å‰Šé™¤: **{phone_removed}** ä»¶\n"
                f"- é‡è¤‡ï¼ˆé›»è©± digitsä¸€è‡´ï¼‰å‰Šé™¤: **{dup_removed}** ä»¶\n"
            )
            if removal_logs:
                log_df = pd.DataFrame(removal_logs)
                st.dataframe(log_df.head(300), use_container_width=True)
                csv_bytes = log_df.to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    "ğŸ§¾ å‰Šé™¤ãƒ­ã‚°ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=csv_bytes,
                    file_name=f"removal_logs_{filename_no_ext}.csv",
                    mime="text/csv",
                    key=f"removal_log_btn_{file_index}",
                )

        # ===============================
        # template.xlsx ã¸æ›¸ãè¾¼ã¿ï¼ˆOSäº’æ›å¼·åŒ–ï¼‰
        # ===============================
        wb = None
        if template_upload is not None:
            try:
                buf = io.BytesIO(template_upload.read())
                wb = load_workbook(buf)
            except Exception as e:
                st.error(f"âŒ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸ template.xlsx ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                st.stop()
        else:
            app_dir = Path(__file__).resolve().parent
            template_path = app_dir / "template.xlsx"
            if not template_path.exists():
                st.error(
                    f"âŒ template.xlsx ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆæœŸå¾…ãƒ‘ã‚¹: {template_path}ï¼‰ã€‚"
                    "ã€ã“ã“ã§ template.xlsx ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ã†ã€ã‚’é¸ã¶ã‹ã€"
                    "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚"
                )
                st.stop()
            try:
                wb = load_workbook(template_path)
            except Exception as e:
                st.error(f"âŒ template.xlsx ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                st.stop()

        if "å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼" not in wb.sheetnames:
            st.error("âŒ template.xlsx ã«ã€å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼ã€ã¨ã„ã†ã‚·ãƒ¼ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            st.stop()

        sheet_master = wb["å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼"]

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ï¼ˆ2è¡Œç›®ä»¥é™ã®Bã€œEï¼‰ã¨å¡—ã‚Šã‚’ã‚¯ãƒªã‚¢
        for row in sheet_master.iter_rows(min_row=2, max_row=sheet_master.max_row):
            for cell in row[1:5]:  # B(1)ã€œE(4)
                cell.value = None
                cell.fill = PatternFill(fill_type=None)

        # ç‰©æµãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆæ¥­ç¨®ã«ç‰¹å®šèªãŒå«ã¾ã‚Œã‚‹å ´åˆã€Cåˆ—ã‚’èµ¤ãï¼‰
        red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")

        def is_logi(val: str) -> bool:
            v = (val or "").strip()
            return any(word in v for word in highlight_partial)

        # ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿ï¼ˆB=ä¼æ¥­å, C=æ¥­ç¨®, D=ä½æ‰€, E=é›»è©±ï¼‰
        for idx_row, row in df_export.iterrows():
            r = idx_row + 2
            sheet_master.cell(row=r, column=2, value=row["ä¼æ¥­å"])
            sheet_master.cell(row=r, column=3, value=row["æ¥­ç¨®"])
            sheet_master.cell(row=r, column=4, value=row["ä½æ‰€"])
            sheet_master.cell(row=r, column=5, value=row["é›»è©±ç•ªå·"])
            if industry_option == "ç‰©æµæ¥­" and is_logi(row["æ¥­ç¨®"]):
                sheet_master.cell(row=r, column=3).fill = red_fill

        # ===============================
        # é–‹æ‹“å…ˆãƒªã‚¹ãƒˆã‚·ãƒ¼ãƒˆã®ãƒ—ãƒ«ãƒ€ã‚¦ãƒ³ï¼†å°åˆ·ç¯„å›²è¨­å®š
        # ===============================
        if "é–‹æ‹“å…ˆãƒªã‚¹ãƒˆ" in wb.sheetnames:
            sheet_k = wb["é–‹æ‹“å…ˆãƒªã‚¹ãƒˆ"]

            # ãƒ—ãƒ«ãƒ€ã‚¦ãƒ³ï¼ˆãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ï¼‰: Håˆ—ã® H3, H9, H15, ... ã«è¨­å®š
            try:
                dv = DataValidation(
                    type="list",
                    formula1='"-,ã‚¢ãƒ,è¦‹è¾¼ã¿,æ–­ã‚Š,ç•™å®ˆ,æ‹…å½“è€…ä¸åœ¨,ä¸ä½¿ç”¨,å‰Šé™¤ä¾é ¼"',
                    allow_blank=True,
                )
                sheet_k.add_data_validation(dv)

                max_row_k = sheet_k.max_row or 200
                row = 3
                while row <= max_row_k:
                    cell_ref = f"H{row}"
                    dv.add(sheet_k[cell_ref])
                    row += 6
            except Exception:
                # DataValidation ãŒã†ã¾ãè¡Œã‹ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆã‚¨ãƒ©ãƒ¼ã§æ­¢ã‚ãªã„ï¼‰
                pass

            # å°åˆ·ç¯„å›²ã‚’ Aã€œL å…¨è¡Œã«è¨­å®š
            try:
                max_row_k = sheet_k.max_row or 200
                sheet_k.print_area = f"A1:L{max_row_k}"
            except Exception:
                pass

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«åˆ¥ãƒœã‚¿ãƒ³ï¼‰
        output = io.BytesIO()
        wb.save(output)
        output.seek(0)
        st.download_button(
            label=f"ğŸ“¥ æ•´å½¢æ¸ˆã¿ãƒªã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ{filename_no_ext} / template.xlsx åæ˜ ï¼‰",
            data=output,
            file_name=f"{filename_no_ext}ãƒªã‚¹ãƒˆ.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            key=f"download_btn_{file_index}",
        )

else:
    st.info("Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚NGãƒªã‚¹ãƒˆxlsxã¯åŒãƒ•ã‚©ãƒ«ãƒ€ã«ç½®ãã‹ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
