import streamlit as st
import pandas as pd
import re
import io
import os
import unicodedata
from openpyxl import load_workbook
from openpyxl.styles import PatternFill

# =========================
# ãƒšãƒ¼ã‚¸è¨­å®šï¼ã‚¹ã‚¿ã‚¤ãƒ«
# =========================
st.set_page_config(page_title="G-Change Next", layout="wide")
st.markdown("""
    <style>
    h1 { color: #800000; }
    </style>
""", unsafe_allow_html=True)
st.title("ğŸš— G-Change Nextï½œä¼æ¥­æƒ…å ±æ•´å½¢ï¼†NGé™¤å¤–ãƒ„ãƒ¼ãƒ«ï¼ˆVer5.0 å®‰å…¨é…åˆ—è£œæ­£ï¼‹ç”ºåè¾æ›¸å¯¾å¿œï¼‰")

# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆæ­£è¦åŒ–ç³»ï¼‰
# =========================
def nfkc(s: str) -> str:
    return unicodedata.normalize("NFKC", s)

def normalize_text(x) -> str:
    """è»½é‡æ­£è¦åŒ–ï¼šNFKCã€ç©ºç™½ãƒ»å„ç¨®ãƒ€ãƒƒã‚·ãƒ¥çµ±ä¸€ã€å‰å¾Œç©ºç™½é™¤å»"""
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return ""
    s = str(x).replace("\u3000", " ").replace("\xa0", " ")
    s = re.sub(r'[âˆ’â€“â€”â€•ãƒ¼]', '-', s)
    s = nfkc(s).strip()
    return s

def hiragana_to_katakana(s: str) -> str:
    res = []
    for ch in s:
        code = ord(ch)
        if 0x3041 <= code <= 0x3096:
            res.append(chr(code + 0x60))
        else:
            res.append(ch)
    return "".join(res)

COMPANY_SUFFIXES = [
    "æ ªå¼ä¼šç¤¾", "(æ ª)", "ï¼ˆæ ªï¼‰", "æœ‰é™ä¼šç¤¾", "(æœ‰)", "ï¼ˆæœ‰ï¼‰",
    "inc.", "inc", "co.,ltd.", "co.,ltd", "co.ltd.", "co.ltd", "ltd.", "ltd",
    "corp.", "corp", "co.", "co",
    "åˆåŒä¼šç¤¾", "åˆåä¼šç¤¾", "åˆè³‡ä¼šç¤¾"
]

def canonical_company_name(name: str) -> str:
    s = normalize_text(name)
    s = hiragana_to_katakana(s)
    s = s.casefold()
    for suf in sorted(COMPANY_SUFFIXES, key=len, reverse=True):
        s = s.replace(suf.casefold(), "")
    s = re.sub(r"[\s\-â€“â€”â€•â€ãƒ¼ãƒ»/,.Â·ï½¥\(\)ï¼ˆï¼‰\[\]{}ã€ã€‘ï¼†&ï¼‹+_|]", "", s)
    return s

# ---- é›»è©±æŠ½å‡ºãƒ»æ•´å½¢ï¼ˆåŸæ–‡ä¿æŒï¼‹â€œå®‰å…¨â€é…åˆ—è£œæ­£ï¼‰ ----
HYPHENS = "-â€’â€“â€”â€•âˆ’ï¼ãƒ¼â€ï¹£\u2011"  # å„ç¨®ãƒã‚¤ãƒ•ãƒ³ï¼‹éæ”¹è¡Œãƒã‚¤ãƒ•ãƒ³
HYPHENS_CLASS = re.escape(HYPHENS)
PHONE_TOKEN_RE = re.compile(
    rf"(\d{{2,4}}[{HYPHENS_CLASS}]?\d{{2,4}}[{HYPHENS_CLASS}]?\d{{3,4}})"
)

def pick_phone_token_raw(line: str) -> str:
    """è¡Œã‹ã‚‰â€œåŸæ–‡ã®é›»è©±è¡¨è¨˜â€ã ã‘ã‚’æŠœãå‡ºã™ï¼ˆãƒã‚¤ãƒ•ãƒ³ä½ç½®ãƒ»ç¨®é¡ã¯å¤‰æ›´ã—ãªã„ï¼‰"""
    s = str(line or "")
    m = PHONE_TOKEN_RE.search(s)
    return m.group(1).strip() if m else ""

Z2H_HYPHEN = str.maketrans({
    'ï¼':'-','ãƒ¼':'-','â€’':'-','â€“':'-','â€”':'-','â€•':'-','â€':'-',
    '\u2011':'-',   # NON-BREAKING HYPHEN
    'âˆ’':'-',        # U+2212
    'ï¹£':'-',       # U+FE63
    '-':'-',        # ASCII
})

def phone_digits_only(s: str) -> str:
    """æ¯”è¼ƒç”¨ï¼šæ•°å­—ã ã‘æŠ½å‡ºï¼ˆNGç…§åˆãƒ»é‡è¤‡åˆ¤å®šç”¨ï¼‰"""
    return re.sub(r"\D", "", nfkc(str(s or "")))

# â€» normalize_phone ã¯â€œè¦‹ãŸç›®æ•´å½¢â€ç”¨ã ãŒã€ä»Šå›ã®å®‰å…¨æ–¹é‡ã§ã¯ä½¿ç”¨ã—ãªã„ï¼ˆåŸæ–‡â†’å¿…è¦æ™‚ã®ã¿è£œæ­£ï¼‰

def clean_address(address):
    address = normalize_text(address)
    split_pattern = r"[Â·ï½¥ãƒ»]"
    if re.search(split_pattern, address):
        return re.split(split_pattern, address)[-1].strip()
    return address

def extract_industry(line):
    parts = re.split(r"[Â·ãƒ»]", normalize_text(line))
    return parts[-1].strip() if len(parts) > 1 else (normalize_text(line))

# =========================
# å¸‚å¤–å±€ç•ªDBï¼ˆCSVï¼‰ï¼‹ ç”ºåâ†’å¸‚åŒºç”ºæ‘DBï¼ˆä»»æ„ï¼‰
# =========================
AREACODE_CSV = "jp_areacodes.csv"     # å¿…é ˆæ¨å¥¨ï¼šéƒ½é“åºœçœŒ/å¸‚åŒºç”ºæ‘â†’å¸‚å¤–å±€ç•ª
TOWN2CITY_CSV = "jp_town2city.csv"    # ä»»æ„ï¼šç”ºåâ†’å¸‚åŒºç”ºæ‘ï¼ˆKEN_ALL ã‹ã‚‰ç”Ÿæˆï¼‰

_area_rows = []       # (pref, muni, area_code)
_pref_names = []      # ["æ±äº¬éƒ½", ...] é•·ã„é †
_muni_rows = []       # (pref, muni, area_code) muni ã‚ã‚Šã®ã¿ã€munié•·ã„é †
_pref_ac_map = {}     # pref -> area_code
_town_rows = []       # (pref, town_keyword, municipality) towné•·ã„é †

def load_area_code_db():
    global _area_rows, _pref_names, _muni_rows, _pref_ac_map
    _area_rows, _pref_names, _muni_rows, _pref_ac_map = [], [], [], {}
    if not os.path.exists(AREACODE_CSV):
        return False
    df = pd.read_csv(AREACODE_CSV).fillna("")
    for _, r in df.iterrows():
        pref = str(r.get("prefecture", "")).strip()
        muni = str(r.get("municipality", "")).strip()
        ac   = str(r.get("area_code", "")).strip()
        if not ac or not pref:
            continue
        if not ac.startswith("0"):
            ac = "0" + ac
        _area_rows.append((pref, muni, ac))
        if not muni:
            _pref_ac_map[pref] = ac
    _pref_names = sorted(list({_p for _p, _, _ in _area_rows}), key=len, reverse=True)
    _muni_rows  = sorted([(p,m,a) for (p,m,a) in _area_rows if m], key=lambda x: len(x[1]), reverse=True)
    return True

def load_town2city_db():
    global _town_rows
    _town_rows = []
    if not os.path.exists(TOWN2CITY_CSV):
        return False
    df = pd.read_csv(TOWN2CITY_CSV).fillna("")
    rows = []
    for _, r in df.iterrows():
        pref = str(r.get("prefecture","")).strip()
        muni = str(r.get("municipality","")).strip()
        town = str(r.get("town_keyword","")).strip()
        if pref and town and muni:
            rows.append((pref, town, muni))
    # ç”ºåã®æœ€é•·ä¸€è‡´å„ªå…ˆ
    _town_rows = sorted(rows, key=lambda x: len(x[1]), reverse=True)
    return True

_aco_loaded = load_area_code_db()
_t2c_loaded = load_town2city_db()

def _find_pref_in_address(addr: str) -> str:
    if not _aco_loaded or not addr:
        return ""
    s = normalize_text(addr)
    for pref in _pref_names:
        if pref and pref in s:
            return pref
    return ""

def _find_muni_in_address(addr: str, pref: str) -> str:
    if not _aco_loaded or not addr or not pref:
        return ""
    s = normalize_text(addr)
    for p, muni, _ac in _muni_rows:
        if p == pref and muni and (muni in s):
            return muni
    return ""

def _find_muni_by_town(addr: str, pref: str) -> str:
    if not _t2c_loaded or not addr or not pref:
        return ""
    s = normalize_text(addr)
    for p, town, muni in _town_rows:
        if p == pref and town and (town in s):
            return muni
    return ""

def guess_area_code_by_address(addr: str) -> str:
    """ä½æ‰€â†’å¸‚å¤–å±€ç•ªï¼ˆå¸‚åŒºç”ºæ‘æœ€é•·ä¸€è‡´â†’ç”ºåé€†å¼•ãâ†’éƒ½é“åºœçœŒä»£è¡¨ï¼‰ã€‚è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ç©ºã€‚"""
    if not _aco_loaded or not addr:
        return ""
    pref = _find_pref_in_address(addr)
    if not pref:
        return ""
    muni = _find_muni_in_address(addr, pref)
    if not muni:
        muni = _find_muni_by_town(addr, pref)
    if muni:
        # muni è¡ŒãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’å„ªå…ˆ
        for p, m, ac in _area_rows:
            if p == pref and m == muni:
                return ac
    # muni ç„¡ã— â†’ éƒ½é“åºœçœŒä»£è¡¨
    return _pref_ac_map.get(pref, "")

# =========================
# æ—¢å­˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼å®šç¾©ï¼ˆç¾çŠ¶ç¶­æŒï¼‰
# =========================
remove_exact = [
    "ã‚ªãƒ•ã‚£ã‚¹æ©Ÿå™¨ãƒ¬ãƒ³ã‚¿ãƒ«æ¥­", "è¶³å ´ãƒ¬ãƒ³ã‚¿ãƒ«ä¼šç¤¾", "é›»æ°—å·¥", "å»ƒæ£„ç‰©ãƒªã‚µã‚¤ã‚¯ãƒ«æ¥­",
    "ãƒ—ãƒ­ãƒ‘ãƒ³è²©å£²æ¥­è€…", "çœ‹æ¿å°‚é–€åº—", "çµ¦æ°´è¨­å‚™å·¥å ´", "è­¦å‚™æ¥­", "å»ºè¨­ä¼šç¤¾",
    "å·¥å‹™åº—", "å†™çœŸåº—", "äººææ´¾é£æ¥­", "æ•´å‚™åº—", "å€‰åº«", "è‚‰åº—", "ç±³è²©å£²åº—",
    "ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒãƒ¼ã‚±ãƒƒãƒˆ", "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã‚µãƒ¼ãƒ“ã‚¹", "å»ºæåº—",
    "è‡ªå‹•è»Šæ•´å‚™å·¥å ´", "è‡ªå‹•è»Šè²©å£²åº—", "è»Šä½“æ•´å‚™åº—", "å”ä¼š/çµ„ç¹”", "å»ºè¨­è«‹è² æ¥­è€…", "é›»å™¨åº—", "å®¶é›»é‡è²©åº—", "å»ºç¯‰ä¼šç¤¾", "ãƒã‚¦ã‚¹ ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¥­", "ç„¼è‚‰åº—",
    "å»ºç¯‰è¨­è¨ˆäº‹å‹™æ‰€","å·¦å®˜","ä½œæ¥­æœåº—","ç©ºèª¿è¨­å‚™å·¥äº‹æ¥­è€…","é‡‘å±ã‚¹ã‚¯ãƒ©ãƒƒãƒ—æ¥­è€…","å®³ç£é§†é™¤ã‚µãƒ¼ãƒ“ã‚¹","ãƒ¢ãƒ¼ã‚¿ãƒ¼ä¿®ç†åº—","ã‚¢ãƒ¼ãƒã‚§ãƒªãƒ¼ã‚·ãƒ§ãƒƒãƒ—","ã‚¢ã‚¹ãƒ™ã‚¹ãƒˆæ¤œæŸ»æ¥­","äº‹å‹™ç”¨å“åº—",
    "æ¸¬é‡å£«","é…ç®¡æ¥­è€…","åŠ´åƒçµ„åˆ","ã‚¬ã‚¹ä¼šç¤¾","ã‚¬ã‚½ãƒªãƒ³ã‚¹ã‚¿ãƒ³ãƒ‰","ã‚¬ãƒ©ã‚¹/ãƒŸãƒ©ãƒ¼åº—","ãƒ¯ã‚¤ãƒŠãƒªãƒ¼","å±‹æ ¹ãµãæ¥­è€…","é«˜ç­‰å­¦æ ¡","é‡‘ç‰©åº—","å²è·¡","å•†å·¥ä¼šè­°æ‰€","æ¸…æƒæ¥­","æ¸…æƒæ¥­è€…","é…ç®¡å·¥"
]
remove_partial = ["è²©å£²åº—", "è²©å£²æ¥­è€…"]

highlight_partial = [
    "é‹è¼¸", "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ã‚¯ã‚¹ã‚µãƒ¼ãƒ“ã‚¹", "å€‰åº«", "è¼¸é€ã‚µãƒ¼ãƒ“ã‚¹",
    "é‹é€ä¼šç¤¾ä¼æ¥­ã®ã‚ªãƒ•ã‚£ã‚¹", "é‹é€ä¼šç¤¾"
]

# =========================
# å…¥åŠ›UI
# =========================
nglist_files = [f for f in os.listdir() if f.endswith(".xlsx") and "NGãƒªã‚¹ãƒˆ" in f]
nglist_options = ["ãªã—"] + [os.path.splitext(f)[0] for f in nglist_files]
selected_nglist = st.selectbox("ğŸ›¡ï¸ ä½¿ç”¨ã™ã‚‹NGãƒªã‚¹ãƒˆã‚’é¸æŠã—ã¦ãã ã•ã„", nglist_options)

# åŸæ–‡ä¿æŒã¯å¸¸ã«å®Ÿæ–½ï¼ˆè¦‹ãŸç›®ã¯ã¾ãšåŸæ–‡ï¼‰ï¼ä¸‹è¨˜ã¯è£œæ­£ã®ON/OFF
enable_area_code_fix = st.checkbox("ä½æ‰€ã¨å¸‚å¤–å±€ç•ªã‚’ç…§åˆã—ã€å¿…è¦ãªå ´åˆã®ã¿é…åˆ—ï¼ˆãƒã‚¤ãƒ•ãƒ³ä½ç½®ï¼‰ã‚’å®‰å…¨ã«è£œæ­£ã™ã‚‹", value=True)

st.markdown("### ğŸ§­ æŠ½å‡ºæ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„")
profile = st.selectbox(
    "æŠ½å‡ºãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«",
    [
        "Googleæ¤œç´¢ãƒªã‚¹ãƒˆï¼ˆç¸¦èª­ã¿ãƒ»é›»è©±ä¸Šä¸‹å‹ï¼‰",
        "ã‚·ã‚´ãƒˆã‚¢ãƒ«ãƒ¯æ¤œç´¢ãƒªã‚¹ãƒˆï¼ˆç¸¦ç©ã¿ãƒ©ãƒ™ãƒ«ï¼‰",
        "æ—¥æœ¬å€‰åº«å”ä¼šãƒªã‚¹ãƒˆï¼ˆ4åˆ—ãƒ»è¤‡æ•°è¡Œãƒ–ãƒ­ãƒƒã‚¯ï¼‰",
    ],
    index=0
)

st.markdown("### ğŸ­ æ¥­ç¨®ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã—ã¦ãã ã•ã„")
industry_option = st.radio(
    "ã©ã®æ¥­ç¨®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«è©²å½“ã—ã¾ã™ã‹ï¼Ÿ",
    ("è£½é€ æ¥­", "ç‰©æµæ¥­", "ãã®ä»–")
)

uploaded_file = st.file_uploader("ğŸ“¤ æ•´å½¢å¯¾è±¡ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["xlsx"])

# =========================
# ä¾‹å¤–ç•ªå·ï¼ˆæºå¸¯ãƒ»ãƒ•ãƒªãƒ¼ãƒ€ã‚¤ãƒ¤ãƒ«ç­‰ï¼‰ã®é…åˆ—è¦å‰‡
# =========================
def format_service_number(digits: str) -> str | None:
    """æºå¸¯ãƒ»IPãƒ»ãƒ•ãƒªãƒ¼ãƒ€ã‚¤ãƒ¤ãƒ«ãƒ»ãƒŠãƒ“ãƒ€ã‚¤ãƒ¤ãƒ«ç­‰ã‚’è¦å‰‡ã§æˆå½¢ï¼ˆæ•°å­—ã¯å¤‰æ›´ã—ãªã„ï¼‰"""
    if not digits:
        return None
    # æºå¸¯/050/020 ãªã©ï¼ˆ11æ¡æƒ³å®šï¼‰â†’ 3-4-4
    if (digits.startswith(("070","080","090","050","020")) and len(digits) == 11):
        return f"{digits[:3]}-{digits[3:7]}-{digits[7:]}"
    # 0120 / 0800 / 0570 / 0990ï¼ˆ10æ¡ï¼‰â†’ 4-3-3
    if len(digits) == 10 and digits.startswith(("0120","0800","0570","0990")):
        return f"{digits[:4]}-{digits[4:7]}-{digits[7:]}"
    return None  # è©²å½“ãªã—

def format_by_area_code(digits: str, area_code: str) -> str | None:
    """å›ºå®šé›»è©±ã‚’å¸‚å¤–å±€ç•ªã®æ¡ã«åˆã‚ã›ã¦é…åˆ—æ•´å½¢ï¼ˆ10æ¡å‰æï¼‰ã€‚"""
    if not digits or not area_code:
        return None
    if len(digits) != 10:
        return None
    if not area_code.startswith("0"):
        area_code = "0" + area_code
    if not digits.startswith(area_code):
        return None  # â˜…å®‰å…¨ç­–ï¼šå¸‚å¤–å±€ç•ªã§å§‹ã¾ã‚‰ãªã„ãªã‚‰æ•´å½¢ã—ãªã„
    rest = 10 - len(area_code)
    if rest <= 0:
        return None
    mid = rest - 4
    if mid <= 0:
        return None
    return f"{digits[:len(area_code)]}-{digits[len(area_code):len(area_code)+mid]}-{digits[-4:]}"

def reformat_phone_by_address(display_phone: str, address: str) -> tuple[str, bool, dict]:
    """
    å…¥åŠ›ï¼šè¡¨ç¤ºç”¨é›»è©±ï¼ˆåŸæ–‡ï¼‰ãƒ»ä½æ‰€
    å‡ºåŠ›ï¼š (è¡¨ç¤ºç”¨é›»è©±ï¼ˆè£œæ­£å¾Œã¾ãŸã¯åŸæ–‡ã®ã¾ã¾ï¼‰, å¤‰æ›´ãƒ•ãƒ©ã‚°, ãƒ­ã‚°æƒ…å ±)
    - æ•°å­—ã¯ä¸€åˆ‡å¤‰æ›´ã›ãšã€é…åˆ—ï¼ˆãƒã‚¤ãƒ•ãƒ³ä½ç½®ï¼‰ã ã‘èª¿æ•´
    - æºå¸¯/ãƒ•ãƒªãƒ¼ãƒ€ã‚¤ãƒ¤ãƒ«ç­‰ã¯å€‹åˆ¥è¦å‰‡ã§æ•´å½¢
    - å›ºå®šé›»è©±ã¯ä½æ‰€â†’å¸‚å¤–å±€ç•ªã§æ•´å½¢ï¼ˆâ˜…digits ãŒå¸‚å¤–å±€ç•ªã§å§‹ã¾ã‚‹ã¨ãã ã‘ï¼‰
    """
    raw = str(display_phone or "")
    digits = phone_digits_only(raw)
    log = {
        "before": raw,
        "after": raw,
        "reason": "",
        "address": address or "",
        "area_code_used": ""
    }
    if len(digits) < 9:
        return raw, False, log  # æ¡ä¸è¶³ãƒ»ä¸æ˜ç­ã¯è§¦ã‚‰ãªã„

    # 1) ç‰¹æ®Šç•ªå·ï¼ˆæºå¸¯/ãƒ•ãƒªãƒ¼ãƒ€ã‚¤ãƒ¤ãƒ«ç­‰ï¼‰
    svc = format_service_number(digits)
    if svc is not None:
        if svc != raw:
            log.update({"after": svc, "reason": "service-format"})
            return svc, True, log
        return raw, False, log

    # 2) å›ºå®šé›»è©±ï¼ˆ10æ¡ãŒåŸºæœ¬ï¼‰ã‚’ä½æ‰€â†’å¸‚å¤–å±€ç•ªã§â€œå®‰å…¨ã«â€é…åˆ—è£œæ­£
    if len(digits) == 10 and digits.startswith("0"):
        ac = guess_area_code_by_address(address or "")
        if ac:
            fixed = format_by_area_code(digits, ac)  # å†…éƒ¨ã§ startswith(ac) ã‚’ãƒã‚§ãƒƒã‚¯
            if fixed and fixed != raw:
                log.update({"after": fixed, "reason": "area-code-format", "area_code_used": ac})
                return fixed, True, log

    return raw, False, log

# =========================
# æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆ3æ–¹å¼ï¼‹å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆï¼‰
# =========================
def extract_google_vertical(lines):
    """Googleç¸¦å‹ã€‚é›»è©±ã¯â€œåŸæ–‡ã®ã¾ã¾â€æŠ½å‡ºï¼ˆè£œæ­£ã¯å¾Œæ®µã§ä¸€æ‹¬é©ç”¨ï¼‰ã€‚"""
    results = []
    rows = [normalize_text(l) for l in lines if normalize_text(l)]
    address_keywords = ["éƒ½","é“","åºœ","çœŒ","å¸‚","åŒº","ç”º","æ‘"]
    company_keywords = ["æ ªå¼ä¼šç¤¾","æœ‰é™ä¼šç¤¾","åˆåŒä¼šç¤¾","åˆåä¼šç¤¾","åˆè³‡ä¼šç¤¾","(æ ª)","ï¼ˆæ ªï¼‰"]
    for i, line in enumerate(rows):
        raw_token = pick_phone_token_raw(line)
        if not raw_token:
            continue
        phone_display = raw_token  # åŸæ–‡ä¿æŒ
        address = ""
        industry = ""
        company = ""
        for j in range(i - 1, -1, -1):
            if any(k in rows[j] for k in address_keywords):
                address = rows[j]
                for k in range(j - 1, -1, -1):
                    if any(c in rows[k] for c in company_keywords):
                        company = rows[k]
                        if k + 1 < j:
                            industry = extract_industry(rows[k + 1])
                        break
                break
        results.append([company, industry, address, phone_display])
    return pd.DataFrame(results, columns=["ä¼æ¥­å", "æ¥­ç¨®", "ä½æ‰€", "é›»è©±ç•ªå·"])

def extract_shigoto_arua(df_like: pd.DataFrame) -> pd.DataFrame:
    """å·¦ï¼šãƒ©ãƒ™ãƒ«/ä¼æ¥­åã€å³ï¼šå€¤ã€‚é›»è©±ã¯åŸæ–‡ä¿æŒï¼ˆè£œæ­£ã¯å¾Œæ®µï¼‰ã€‚"""
    df = df_like.copy()
    if df.columns.size > 2:
        df = df.iloc[:, :2]
    df.columns = ["col0", "col1"]
    df["col0"] = df["col0"].map(normalize_text)
    df["col1"] = df["col1"]  # åŸæ–‡ä¿æŒ

    def norm_label(s: str) -> str:
        s = (s or "")
        s = re.sub(r"[ï¼š:]\s*$", "", s)
        return s

    label_to_field = {
        "ä½æ‰€": "ä½æ‰€",
        "æ‰€åœ¨åœ°": "ä½æ‰€",
        "æœ¬ç¤¾æ‰€åœ¨åœ°": "ä½æ‰€",
        "é›»è©±": "é›»è©±ç•ªå·",
        "é›»è©±ç•ªå·": "é›»è©±ç•ªå·",
        "TEL": "é›»è©±ç•ªå·",
        "Tel": "é›»è©±ç•ªå·",
        "tel": "é›»è©±ç•ªå·",
        "æ¥­ç¨®": "æ¥­ç¨®",
        "äº‹æ¥­å†…å®¹": "æ¥­ç¨®",
        "ç”£æ¥­åˆ†é¡": "æ¥­ç¨®",
        "è£½é€ æ¥­ç¨®": "æ¥­ç¨®",
    }

    non_company_labels = set([
        "ä½æ‰€","æ‰€åœ¨åœ°","æœ¬ç¤¾æ‰€åœ¨åœ°",
        "é›»è©±","é›»è©±ç•ªå·","TEL","Tel","tel",
        "FAX","ï¼¦ï¼¡ï¼¸",
        "è³‡æœ¬é‡‘","è³‡æœ¬é‡‘ï¼ˆåƒå††ï¼‰","è³‡æœ¬é‡‘(åƒå††)",
        "å¾“æ¥­å“¡æ•°","è¨­ç«‹å¹´æœˆ",
        "æ¥­ç¨®","äº‹æ¥­å†…å®¹","ç”£æ¥­åˆ†é¡","è£½é€ æ¥­ç¨®"
    ])

    current = {"ä¼æ¥­å": "", "ä½æ‰€": "", "é›»è©±ç•ªå·": "", "æ¥­ç¨®": ""}
    out = []

    def flush_current():
        if current["ä¼æ¥­å"]:
            phone_val = str(current["é›»è©±ç•ªå·"]).strip()
            out.append([current["ä¼æ¥­å"], current["æ¥­ç¨®"], current["ä½æ‰€"], phone_val])
        current.update({"ä¼æ¥­å":"","ä½æ‰€":"","é›»è©±ç•ªå·":"","æ¥­ç¨®":""})

    for _, row in df.iterrows():
        left = norm_label(row["col0"])
        right = row["col1"]

        if left and (right == "" or right is None) and left not in non_company_labels:
            if current["ä¼æ¥­å"]:
                flush_current()
            current["ä¼æ¥­å"] = left
            continue

        if left in label_to_field and right is not None:
            key = label_to_field[left]
            if key == "ä½æ‰€":
                current["ä½æ‰€"] = clean_address(right)
            elif key == "é›»è©±ç•ªå·":
                current["é›»è©±ç•ªå·"] = right
            elif key == "æ¥­ç¨®":
                current["æ¥­ç¨®"] = extract_industry(right)
            continue

    if current["ä¼æ¥­å"]:
        flush_current()

    return pd.DataFrame(out, columns=["ä¼æ¥­å", "æ¥­ç¨®", "ä½æ‰€", "é›»è©±ç•ªå·"])

def extract_warehouse_association(df_like: pd.DataFrame) -> pd.DataFrame:
    """æ—¥æœ¬å€‰åº«å”ä¼šï¼š4åˆ—ãƒ–ãƒ­ãƒƒã‚¯ã€‚é›»è©±ã¯åŸæ–‡ä¿æŒï¼ˆè£œæ­£ã¯å¾Œæ®µï¼‰ã€‚"""
    df = df_like.copy()
    if df.shape[1] < 2:
        return pd.DataFrame(columns=["ä¼æ¥­å","æ¥­ç¨®","ä½æ‰€","é›»è©±ç•ªå·"])
    while df.shape[1] < 4:
        df[f"__pad{df.shape[1]}"] = ""
    df = df.iloc[:, :4]
    df.columns = ["c0","c1","c2","c3"]
    for c in df.columns:
        df[c] = df[c].map(normalize_text)

    FACILITY_KEYWORDS = ["å–¶æ¥­æ‰€","ã‚»ãƒ³ã‚¿ãƒ¼","æ”¯åº—","äº‹æ¥­æ‰€","å‡ºå¼µæ‰€","å€‰åº«","ãƒ‡ãƒ","ç‰©æµã‚»ãƒ³ã‚¿ãƒ¼","é…é€ã‚»ãƒ³ã‚¿ãƒ¼"]
    LEGAL_KEYWORDS = ["æ ªå¼ä¼šç¤¾","ï¼ˆæ ªï¼‰","(æ ª)","æœ‰é™ä¼šç¤¾","åˆåŒä¼šç¤¾","åˆåä¼šç¤¾","åˆè³‡ä¼šç¤¾","Inc","INC","Co.,","CO.,","Ltd","LTD","Corp","CORP"]

    def looks_like_company(name: str) -> bool:
        if not name: return False
        if any(k in name for k in FACILITY_KEYWORDS): return False
        if any(k in name for k in LEGAL_KEYWORDS): return True
        return False

    out = []
    current = {"ä¼æ¥­å":"", "ä½æ‰€":"", "é›»è©±ç•ªå·":"", "æ¥­ç¨®_set":set()}

    def flush_current():
        if current["ä¼æ¥­å"]:
            raw = str(current["é›»è©±ç•ªå·"]).strip()
            phone_display = pick_phone_token_raw(raw) or raw
            industry = "ãƒ»".join([x for x in current["æ¥­ç¨®_set"] if x]) or ""
            out.append([current["ä¼æ¥­å"], industry, current["ä½æ‰€"], phone_display])
        current.update({"ä¼æ¥­å":"", "ä½æ‰€":"", "é›»è©±ç•ªå·":"", "æ¥­ç¨®_set":set()})

    tel_re = re.compile(r"(TEL|ï¼´ï¼¥ï¼¬)\s*([0-9ï¼-ï¼™\-ï½°ãƒ¼ï¼]+)", re.IGNORECASE)
    zip_re = re.compile(r"^ã€’\s*\d{3}-?\d{4}")

    for _, row in df.iterrows():
        c0, c1, c2, c3 = row["c0"], row["c1"], row["c2"], row["c3"]

        if c0 and looks_like_company(c0):
            if current["ä¼æ¥­å"] and c0 != current["ä¼æ¥­å"]:
                flush_current()
            current["ä¼æ¥­å"] = c0

        if c1:
            if zip_re.search(c1):
                current["ä½æ‰€"] = c1 if not current["ä½æ‰€"] else f"{current['ä½æ‰€']} {c1}"
            else:
                if any(tok in c1 for tok in ["éƒ½","é“","åºœ","çœŒ","å¸‚","åŒº","ç”º","æ‘"]):
                    current["ä½æ‰€"] = c1 if not current["ä½æ‰€"] else f"{current['ä½æ‰€']} {c1}"

        if c2:
            m = tel_re.search(c2)
            if m and not current["é›»è©±ç•ªå·"]:
                current["é›»è©±ç•ªå·"] = m.group(2)

        if c3:
            current["æ¥­ç¨®_set"].add(extract_industry(c3))

    if current["ä¼æ¥­å"]:
        flush_current()

    return pd.DataFrame(out, columns=["ä¼æ¥­å","æ¥­ç¨®","ä½æ‰€","é›»è©±ç•ªå·"])

# =========================
# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def remove_empty_rows(df):
    return df[~((df["ä¼æ¥­å"] == "") & (df["æ¥­ç¨®"] == "") & (df["ä½æ‰€"] == "") & (df["é›»è©±ç•ªå·"] == ""))]

# =========================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =========================
if uploaded_file:
    filename_no_ext = os.path.splitext(uploaded_file.name)[0]
    xl = pd.ExcelFile(uploaded_file)

    # === å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼å„ªå…ˆï¼ˆB:ä¼æ¥­å/C:æ¥­ç¨®/D:ä½æ‰€/E:é›»è©±ï¼‰ ===
    if "å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼" in xl.sheet_names:
        df_raw = pd.read_excel(xl, sheet_name="å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼", header=None).fillna("")
        # è¡¨ç¤ºã¯ã¾ãšâ€œåŸæ–‡ã®ã¾ã¾â€
        disp_phone_series = df_raw.iloc[:, 4].astype(str).map(lambda v: str(v).strip())
        result_df = pd.DataFrame({
            "ä¼æ¥­å": df_raw.iloc[:, 1].astype(str).map(normalize_text),
            "æ¥­ç¨®": df_raw.iloc[:, 2].astype(str).map(normalize_text),
            "ä½æ‰€": df_raw.iloc[:, 3].astype(str).map(clean_address),
            "é›»è©±ç•ªå·": disp_phone_series
        })
    else:
        # --- æŠ½å‡ºï¼ˆå›ºå®šãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ ---
        if profile == "Googleæ¤œç´¢ãƒªã‚¹ãƒˆï¼ˆç¸¦èª­ã¿ãƒ»é›»è©±ä¸Šä¸‹å‹ï¼‰":
            df = pd.read_excel(uploaded_file, header=None).fillna("")
            lines = df.iloc[:, 0].tolist()
            result_df = extract_google_vertical(lines)

        elif profile == "ã‚·ã‚´ãƒˆã‚¢ãƒ«ãƒ¯æ¤œç´¢ãƒªã‚¹ãƒˆï¼ˆç¸¦ç©ã¿ãƒ©ãƒ™ãƒ«ï¼‰":
            df0 = pd.read_excel(xl, sheet_name=xl.sheet_names[0], header=None).fillna("")
            result_df = extract_shigoto_arua(df0)

        else:  # æ—¥æœ¬å€‰åº«å”ä¼š
            df0 = pd.read_excel(xl, sheet_name=xl.sheet_names[0], header=None).fillna("")
            result_df = extract_warehouse_association(df0)

    # --- æ­£è¦åŒ–ï¼†æ¯”è¼ƒã‚­ãƒ¼ ---
    result_df = result_df.fillna("")
    result_df["__company_canon"] = result_df["ä¼æ¥­å"].map(canonical_company_name)
    result_df["__phone_digits"]  = result_df["é›»è©±ç•ªå·"].map(phone_digits_only)

    # === ä½æ‰€Ã—å¸‚å¤–å±€ç•ªâ€œå®‰å…¨â€é…åˆ—è£œæ­£ï¼ˆæŠ½å‡ºå¾Œã®ä¸€æ‹¬å¾Œå‡¦ç†ï¼‰ ===
    area_fix_logs = []
    if enable_area_code_fix:
        new_phones = []
        for idx, row in result_df.iterrows():
            new_val, changed, log = reformat_phone_by_address(row["é›»è©±ç•ªå·"], row["ä½æ‰€"])
            new_phones.append(new_val)
            if changed:
                area_fix_logs.append({
                    "row": idx,
                    **log
                })
        result_df["é›»è©±ç•ªå·"] = new_phones
        # æ¯”è¼ƒã‚­ãƒ¼ã‚’å†ç”Ÿæˆï¼ˆé…åˆ—è£œæ­£å¾Œã®æ•°å­—ã«åŸºã¥ãï¼‰
        result_df["__phone_digits"] = result_df["é›»è©±ç•ªå·"].map(phone_digits_only)

    # --- æ¥­ç¨®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆç¾çŠ¶ç¶­æŒï¼‰ ---
    removed_by_industry = 0
    styled_df = None
    if industry_option == "è£½é€ æ¥­":
        before = len(result_df)
        result_df = result_df[~result_df["æ¥­ç¨®"].isin(remove_exact)]
        if remove_partial:
            pat = "|".join(map(re.escape, remove_partial))
            result_df = result_df[~result_df["æ¥­ç¨®"].str.contains(pat, na=False)]
        removed_by_industry = before - len(result_df)
        st.warning(f"ğŸ­ è£½é€ æ¥­ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨ï¼š{removed_by_industry}ä»¶ã‚’é™¤å¤–ã—ã¾ã—ãŸ")

    elif industry_option == "ç‰©æµæ¥­":
        def highlight_logistics(val):
            v = val or ""
            return "background-color: red" if any(word in v for word in highlight_partial) else ""
        styled_df = result_df.style.applymap(highlight_logistics, subset=["æ¥­ç¨®"])
        st.info("ğŸšš æ¥­ç¨®ãŒä¸€è‡´ã—ãŸã‚»ãƒ«ã‚’èµ¤ããƒã‚¤ãƒ©ã‚¤ãƒˆã—ã¦ã„ã¾ã™ï¼ˆå‡ºåŠ›ã«ã‚‚åæ˜ ï¼‰")

    # --- NGãƒªã‚¹ãƒˆï¼é‡è¤‡å‰Šé™¤ï¼ã‚µãƒãƒªãƒ¼ ---
    removal_logs = []
    company_removed = 0
    phone_removed = 0

    if selected_nglist != "ãªã—":
        ng_path = f"{selected_nglist}.xlsx"
        if not os.path.exists(ng_path):
            st.error(f"âŒ é¸æŠã•ã‚ŒãŸNGãƒªã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼š{ng_path}")
            st.stop()
        ng_df = pd.read_excel(ng_path).fillna("")
        if ng_df.shape[1] < 2:
            st.error("âŒ NGãƒªã‚¹ãƒˆã¯2åˆ—ä»¥ä¸Šå¿…è¦ã§ã™ï¼ˆä¼æ¥­åã€é›»è©±ç•ªå·ï¼‰")
            st.stop()

        ng_df["__ng_company_canon"] = ng_df.iloc[:, 0].map(canonical_company_name)
        ng_df["__ng_phone_digits"]  = ng_df.iloc[:, 1].map(phone_digits_only)

        ng_company_keys = ng_df["__ng_company_canon"].tolist()
        ng_phone_set    = set([p for p in ng_df["__ng_phone_digits"].tolist() if p])

        before = len(result_df)
        def hit_ng_company(canon_name: str):
            for ng in ng_company_keys:
                if ng and canon_name and (ng in canon_name or canon_name in ng):
                    return ng
            return None

        hit_indices = []
        for idx, row in result_df.iterrows():
            ng_key = hit_ng_company(row["__company_canon"])
            if ng_key:
                removal_logs.append({
                    "reason": "ng-company",
                    "source_company": row["ä¼æ¥­å"],
                    "source_phone": row["é›»è©±ç•ªå·"],
                    "match_key": row["__company_canon"],
                    "ng_hit": ng_key
                })
                hit_indices.append(idx)
        if hit_indices:
            result_df = result_df.drop(index=hit_indices)
        company_removed = before - len(result_df)

        before = len(result_df)
        hits = result_df["__phone_digits"].isin(ng_phone_set)
        if hits.any():
            for idx, row in result_df[hits].iterrows():
                removal_logs.append({
                    "reason": "ng-phone",
                    "source_company": row["ä¼æ¥­å"],
                    "source_phone": row["é›»è©±ç•ªå·"],
                    "match_key": row["__phone_digits"],
                    "ng_hit": row["__phone_digits"]
                })
            result_df = result_df[~hits]
        phone_removed = before - len(result_df)

    before = len(result_df)
    dup_mask = result_df["__phone_digits"].ne("").astype(bool) & result_df["__phone_digits"].duplicated(keep="first")
    if dup_mask.any():
        for idx, row in result_df[dup_mask].iterrows():
            removal_logs.append({
                "reason": "phone-duplicate",
                "source_company": row["ä¼æ¥­å"],
                "source_phone": row["é›»è©±ç•ªå·"],
                "match_key": row["__phone_digits"],
                "ng_hit": ""
            })
        result_df = result_df[~dup_mask]
    removed_by_dedup = before - len(result_df)

    # --- ç©ºè¡Œé™¤å»ãƒ»ä¸¦ã¹æ›¿ãˆï¼ˆç©ºé›»è©±ã¯æœ€å¾Œï¼‰ ---
    result_df = remove_empty_rows(result_df)
    result_df["_phdigits"] = result_df["__phone_digits"]
    result_df["_is_empty_phone"] = (result_df["_phdigits"] == "")
    result_df = result_df.sort_values(by=["_is_empty_phone", "_phdigits", "ä¼æ¥­å"]).drop(columns=["_phdigits","_is_empty_phone"])
    result_df = result_df.reset_index(drop=True)

    # --- ç”»é¢è¡¨ç¤º ---
    st.success(f"âœ… æ•´å½¢å®Œäº†ï¼š{len(result_df)}ä»¶ã®ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    if industry_option == "ç‰©æµæ¥­" and styled_df is not None:
        st.dataframe(styled_df, use_container_width=True)
    else:
        st.dataframe(result_df[["ä¼æ¥­å","æ¥­ç¨®","ä½æ‰€","é›»è©±ç•ªå·"]], use_container_width=True)

    # --- ã‚µãƒãƒªãƒ¼ï¼‹å‰Šé™¤ãƒ­ã‚°DL ---
    area_fixed_count = len(area_fix_logs)
    with st.expander("ğŸ“Š å®Ÿè¡Œã‚µãƒãƒªãƒ¼ï¼ˆè©³ç´°ï¼‰"):
        st.markdown(f"""
- å¸‚å¤–å±€ç•ªã«åŸºã¥ãâ€œå®‰å…¨â€é…åˆ—è£œæ­£: **{area_fixed_count}** ä»¶  
- ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é™¤å¤–ï¼ˆè£½é€ æ¥­ å®Œå…¨ä¸€è‡´ï¼‹ä¸€éƒ¨éƒ¨åˆ†ä¸€è‡´ï¼‰: **{removed_by_industry}** ä»¶  
- NGï¼ˆä¼æ¥­åãƒ»éƒ¨åˆ†ä¸€è‡´ï¼‰å‰Šé™¤: **{company_removed}** ä»¶  
- NGï¼ˆé›»è©±ãƒ»æ•°å­—ä¸€è‡´ï¼‰å‰Šé™¤: **{phone_removed}** ä»¶  
- é‡è¤‡ï¼ˆé›»è©±ãƒ»æ•°å­—ä¸€è‡´ï¼‰å‰Šé™¤: **{removed_by_dedup}** ä»¶  
""")
        if area_fix_logs:
            fix_df = pd.DataFrame(area_fix_logs)
            st.dataframe(fix_df.head(100), use_container_width=True)
            st.download_button(
                "ğŸ§¾ å¸‚å¤–å±€ç•ªè£œæ­£ãƒ­ã‚°ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=fix_df.to_csv(index=False).encode("utf-8-sig"),
                file_name="area_fix_logs.csv",
                mime="text/csv"
            )
        if removal_logs:
            log_df = pd.DataFrame(removal_logs)
            st.dataframe(log_df.head(100), use_container_width=True)
            st.download_button(
                "ğŸ§¾ NG/é‡è¤‡ã®å‰Šé™¤ãƒ­ã‚°ã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=log_df.to_csv(index=False).encode("utf-8-sig"),
                file_name="removal_logs.csv",
                mime="text/csv"
            )

    # --- Excelå‡ºåŠ›ï¼ˆç‰©æµãƒã‚¤ãƒ©ã‚¤ãƒˆã‚‚åæ˜ ï¼‰ ---
    template_file = "template.xlsx"
    if not os.path.exists(template_file):
        st.error("âŒ template.xlsx ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
        st.stop()

    workbook = load_workbook(template_file)
    if "å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼" not in workbook.sheetnames:
        st.error("âŒ template.xlsx ã«ã€å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼ã€ã¨ã„ã†ã‚·ãƒ¼ãƒˆãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        st.stop()

    sheet = workbook["å…¥åŠ›ãƒã‚¹ã‚¿ãƒ¼"]
    for row in sheet.iter_rows(min_row=2, max_row=sheet.max_row):
        for cell in row[1:5]:
            cell.value = None
            cell.fill = PatternFill(fill_type=None)

    red_fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")

    def is_logi(val: str) -> bool:
        v = val or ""
        return any(word in v for word in highlight_partial)

    for idx, row in result_df.iterrows():
        r = idx + 2
        sheet.cell(row=r, column=2, value=row["ä¼æ¥­å"])
        sheet.cell(row=r, column=3, value=row["æ¥­ç¨®"])
        sheet.cell(row=r, column=4, value=row["ä½æ‰€"])
        sheet.cell(row=r, column=5, value=row["é›»è©±ç•ªå·"])
        if industry_option == "ç‰©æµæ¥­" and is_logi(row["æ¥­ç¨®"]):
            sheet.cell(row=r, column=3).fill = red_fill

    output = io.BytesIO()
    workbook.save(output)
    output.seek(0)

    st.download_button(
        label="ğŸ“¥ æ•´å½¢æ¸ˆã¿ãƒªã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=output,
        file_name=f"{filename_no_ext}ãƒªã‚¹ãƒˆ.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )
else:
    st.info("template.xlsxã€jp_areacodes.csvï¼ˆæ¨å¥¨ï¼‰ã€å¿…è¦ã«å¿œã˜ã¦ jp_town2city.csv ã‚’åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«ç½®ã„ã¦ã‹ã‚‰ã€Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
